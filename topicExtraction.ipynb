{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "\n",
    "In this Project, you will bring together many of the tools and techniques that you have learned throughout this course into a final project. You can choose from many different paths to get to the solution. \n",
    "\n",
    "### Business scenario\n",
    "\n",
    "You work for a training organization that recently developed an introductory course about machine learning (ML). The course includes more than 40 videos that cover a broad range of ML topics. You have been asked to create an application that will students can use to quickly locate and view video content by searching for topics and key phrases.\n",
    "\n",
    "You have downloaded all of the videos to an Amazon Simple Storage Service (Amazon S3) bucket. Your assignment is to produce a dashboard that meets your supervisorâ€™s requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project steps\n",
    "\n",
    "To complete this project, you will follow these steps:\n",
    "\n",
    "1. [Transcribing the videos](#2.-Transcribing-the-videos)\n",
    "2. [Normalizing the text](#3.-Normalizing-the-text)\n",
    "3. [Extracting key phrases and topics](#4.-Extracting-key-phrases-and-topics)\n",
    "4. [Creating the dashboard](#5.-Creating-the-dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "# !pip install SpeechRecognition==3.1.3\n",
    "# !pip install keybert\n",
    "# !pip install gensim\n",
    "# !pip install keybert\n",
    "# !pip install Wave\n",
    "# !pip install moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transcribing the videos\n",
    " ([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to implement your solution to transcribe the videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import os\n",
    "import subprocess\n",
    "import wave, math, contextlib\n",
    "from moviepy.editor import AudioFileClip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_transcript_file(source_file,destination_file):\n",
    "    audioclip = AudioFileClip(source_file)\n",
    "    audioclip.write_audiofile(f\"{destination_file}.wav\")\n",
    "    with contextlib.closing(wave.open(f'{destination_file}.wav','r')) as f:\n",
    "        frames = f.getnframes()\n",
    "        rate = f.getframerate()\n",
    "        duration = frames / float(rate)\n",
    "    \n",
    "    total_duration = math.ceil(duration / 60)\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    for i in range(0, total_duration):\n",
    "        with sr.WavFile(f'{destination_file}.wav') as source:\n",
    "            audio = r.record(source, offset=i*60, duration=60)\n",
    "        f = open(f\"{destination_file}.txt\", \"a\")\n",
    "#         print(r.recognize_google(audio,language='en-US',show_all = True ))\n",
    "        f.write(r.recognize_google(audio,language='en-CA'))\n",
    "        f.write(\" \")\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod03_Sect03_part1.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Sect03_part1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_Sect04_part2.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Sect04_part2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_Sect05.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Sect05.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod04_Sect01.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod04_Sect01.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_Sect01.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Sect01.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod07_Sect01.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod07_Sect01.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod05_WrapUp_ver2.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod05_WrapUp_ver2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod05_Intro.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod05_Intro.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_Sect03_part3.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Sect03_part3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod06_Intro.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod06_Intro.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod02_Sect04.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod02_Sect04.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod06_Sect01.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod06_Sect01.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_Sect06.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Sect06.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod06_Sect02.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod06_Sect02.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_Sect04_part1.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Sect04_part1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_Sect07_part1.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Sect07_part1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod01_Course Overview.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod01_Course Overview.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_Sect02_part3.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Sect02_part3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_Intro.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Intro.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod06_WrapUp.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod06_WrapUp.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod02_Sect02.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod02_Sect02.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod02_Sect01.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod02_Sect01.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_WrapUp.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_WrapUp.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod02_Sect05.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod02_Sect05.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod02_Sect03.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod02_Sect03.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod04_WrapUp.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod04_WrapUp.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod05_Sect03_part3.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod05_Sect03_part3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod02_WrapUp.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod02_WrapUp.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Mod03_Sect02_part2.mp4\n",
      "MoviePy - Writing audio in ./transcriptions/Mod03_Sect02_part2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize recognizer class (for recognizing the speech)\n",
    "rootdir = './data'\n",
    "desitnation_folder = './transcriptions'\n",
    "for fname in os.listdir(rootdir):\n",
    "    print(fname)\n",
    "    sourcename = os.path.join(rootdir,fname)\n",
    "    dest_name =  os.path.join(desitnation_folder,fname.split('.')[0]) \n",
    "    try:\n",
    "        get_transcript_file(sourcename, dest_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalizing the text\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to perform any text normalization steps that are necessary for your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import nltk\n",
    "import gensim \n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer/code here\\\n",
    "def clean_text(lst):\n",
    "    cleaned_text = []\n",
    "    stopword = stopwords.words(\"english\")\n",
    "    \n",
    "    ## Text Cleaning (Removing Punctuations, Stopwords, Tokenization and Lemmatization)\n",
    "    for text in lst:\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^\\w ]+', \"\", text)\n",
    "        text = \" \".join([lemmatizer.lemmatize(word,pos='v') for word in word_tokenize(text) if not word in set(stopword) and len(word)>3])\n",
    "        if text != '':\n",
    "            cleaned_text.append(text)\n",
    "            \n",
    "    norm_text = ' '.join(cleaned_text)\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " welcome back academy machine learn module go work entire machine learn pipeline use amazon sage maker module discuss typical process handle machine learn problem machine learn pipeline apply many machine learn problems focus supervise learn process learn module adapt type machine learn well large module well cover material module youll able formulate problem business request obtain secure data machine learn build jupiter notebook use amazon sage maker outline process evaluate data explain need preprocessed open source tool examine preprocess data amazon sagemaker train host machine learn model cross validation test performance machine learn model host model inference finally create amazon sagemaker hyperparameter tune optimize model effectiveness ready start next video welcome back academy machine learn module go work entire machine learn pipeline use amazon sage maker module discuss typical process handle machine learn problem machine learn pipeline apply many machine learn problems focus supervise learn process learn module adapt type machine learn well large module well cover material module youll able formulate problem business request obtain secure data machine learn build jupiter notebook use amazon sage maker outline process evaluate data explain need preprocessed open source tool examine preprocess data amazon sagemaker train host machine learn model cross validation test performance machine learn model host model inference finally create amazon sagemaker hyperparameter tune optimize model effectiveness ready start next video welcome back section go discuss challenge machine learn youll come across many challenge machine learn poor quality inconsistent data available significant portion get access generate enough good data representative problem want solve issue watch overfitting model data although mostly data science experience staff team data scientists cost effective management support use machine learn business landscape look like problems complex formulate machine learn problem result model explain business cant explain might adopt whats cost build update operate machine learn solution finally technology business unit access data thats need data secure meet regulatory requirements tool frameworks use solution integrate systems important question successful youll need able answer address many machine learn problems solve today use exist model without substantial machine learn knowledge weve already talk manage service machine learn sophisticate machine learn capabilities applications basic developer skills call apis prebuilt model adapt example yolo mean look yolo popular computer vision model addition scenarios marketplace youd prefer model service independent software vendors instead develop takeaways section first youll face many machine learn challenge biggest ones directly influence relate data consider manage service solve machine learn problems within domains support use amazon recognition computer vision problems thats section well next video welcome back section go discuss challenge machine learn youll come across many challenge machine learn poor quality inconsistent data available significant portion get access generate enough good data representative problem want solve issue watch overfitting model data although mostly data science experience staff team data scientists cost effective management support use machine learn business landscape look like problems complex formulate machine learn problem result model explain business cant explain might adopt whats cost build update operate machine learn solution finally technology business unit access data thats need data secure meet regulatory requirements tool frameworks use solution integrate systems important question successful youll need able answer address many machine learn problems solve today use exist model without substantial machine learn knowledge weve already talk manage service machine learn sophisticate machine learn capabilities applications basic developer skills call apis prebuilt model adapt example yolo mean look yolo popular computer vision model addition scenarios marketplace youd prefer model service independent software vendors instead develop takeaways section first youll face many machine learn challenge biggest ones directly influence relate data consider manage service solve machine learn problems within domains support use amazon recognition computer vision problems thats section well next video welcome back well continue explore feature engineer review clean data addition convert string data numerical data youll need clean data several potential problem areas encode string data make sure string consistent youll also need make sure variables consistent scale example variable describe number doors scale probably eight another variable describe number cars particular type sell state california scale probably thousands data items might also capture variable single value instance suppose data include variables combine safety maintenance single variable safe high maintenance need train machine learn system variables also split single variable separate variables might also encounter data set miss data variables data set include outliers well cover techniques deal situations section might find data miss example columns data could miss data data collection error maybe data wasnt collect particular feature data collection process underway miss data make difficult accurately interpret relationship relate feature target variable regardless data end miss important deal issue unfortunately learn algorithms cant handle miss value automatically youll need human intelligence update miss value data thats meaningful relevant problem python libraries data manipulation include function find miss data decide drop impute miss value question answer part better understand value come miss first place much data miss value represent within larger data instance miss value randomly spread throughout data dont represent larger portion respective column case imputation likely better option contrast column large percentage miss value case drop entire column would imputation decide drop row miss data builtin function example pandas drop function drop row miss data drop specific data value use subset alternative drop miss value impute value miss value different ways impute miss value categorical value miss value usually replace mean median frequent value numerical continuous variables miss value usually replace mean median compute single miss data know univariate also multiple row know multivariate well look univariate example psychic learn pewter function use impute miss value fairly small data miss value miss value impute strategy mean first calculate mean mean three youll impute mean value miss value data libraries include impute package provide complex ways impute data examples include nearest neighbor soft puke multiple imputation chain equations others thats part section well part three well review work outliers data welcome back well continue explore feature engineer review clean data addition convert string data numerical data youll need clean data several potential problem areas encode string data make sure string consistent youll also need make sure variables consistent scale example variable describe number doors scale probably eight another variable describe number cars particular type sell state california scale probably thousands data items might also capture variable single value instance suppose data include variables combine safety maintenance single variable safe high maintenance need train machine learn system variables also split single variable separate variables might also encounter data set miss data variables data set include outliers well cover techniques deal situations section might find data miss example columns data could miss data data collection error maybe data wasnt collect particular feature data collection process underway miss data make difficult accurately interpret relationship relate feature target variable regardless data end miss important deal issue unfortunately learn algorithms cant handle miss value automatically youll need human intelligence update miss value data thats meaningful relevant problem python libraries data manipulation include function find miss data decide drop impute miss value question answer part better understand value come miss first place much data miss value represent within larger data instance miss value randomly spread throughout data dont represent larger portion respective column case imputation likely better option contrast column large percentage miss value case drop entire column would imputation decide drop row miss data builtin function example pandas drop function drop row miss data drop specific data value use subset alternative drop miss value impute value miss value different ways impute miss value categorical value miss value usually replace mean median frequent value numerical continuous variables miss value usually replace mean median compute single miss data know univariate also multiple row know multivariate well look univariate example psychic learn pewter function use impute miss value fairly small data miss value miss value impute strategy mean first calculate mean mean three youll impute mean value miss value data libraries include impute package provide complex ways impute data examples include nearest neighbor soft puke multiple imputation chain equations others thats part section well part three well review work outliers data welcome back well continue explore feature engineer review clean data addition convert string data numerical data youll need clean data several potential problem areas encode string data make sure string consistent youll also need make sure variables consistent scale example variable describe number doors scale probably eight another variable describe number cars particular type sell state california scale probably thousands data items might also capture variable single value instance suppose data include variables combine safety maintenance single variable safe high maintenance need train machine learn system variables also split single variable separate variables might also encounter data set miss data variables data set include outliers well cover techniques deal situations section might find data miss example columns data could miss data data collection error maybe data wasnt collect particular feature data collection process underway miss data make difficult accurately interpret relationship relate feature target variable regardless data end miss important deal issue unfortunately learn algorithms cant handle miss value automatically youll need human intelligence update miss value data thats meaningful relevant problem python libraries data manipulation include function find miss data decide drop impute miss value question answer part better understand value come miss first place much data miss value represent within larger data instance miss value randomly spread throughout data dont represent larger portion respective column case imputation likely better option contrast column large percentage miss value case drop entire column would imputation decide drop row miss data builtin function example pandas drop function drop row miss data drop specific data value use subset alternative drop miss value impute value miss value different ways impute miss value categorical value miss value usually replace mean median frequent value numerical continuous variables miss value usually replace mean median compute single miss data know univariate also multiple row know multivariate well look univariate example psychic learn pewter function use impute miss value fairly small data miss value miss value impute strategy mean first calculate mean mean three youll impute mean value miss value data libraries include impute package provide complex ways impute data examples include nearest neighbor soft puke multiple imputation chain equations others thats part section well part three well review work outliers data welcome back well continue explore feature engineer review clean data addition convert string data numerical data youll need clean data several potential problem areas encode string data make sure string consistent youll also need make sure variables consistent scale example variable describe number doors scale probably eight another variable describe number cars particular type sell state california scale probably thousands data items might also capture variable single value instance suppose data include variables combine safety maintenance single variable safe high maintenance need train machine learn system variables also split single variable separate variables might also encounter data set miss data variables data set include outliers well cover techniques deal situations section might find data miss example columns data could miss data data collection error maybe data wasnt collect particular feature data collection process underway miss data make difficult accurately interpret relationship relate feature target variable regardless data end miss important deal issue unfortunately learn algorithms cant handle miss value automatically youll need human intelligence update miss value data thats meaningful relevant problem python libraries data manipulation include function find miss data decide drop impute miss value question answer part better understand value come miss first place much data miss value represent within larger data instance miss value randomly spread throughout data dont represent larger portion respective column case imputation likely better option contrast column large percentage miss value case drop entire column would imputation decide drop row miss data builtin function example pandas drop function drop row miss data drop specific data value use subset alternative drop miss value impute value miss value different ways impute miss value categorical value miss value usually replace mean median frequent value numerical continuous variables miss value usually replace mean median compute single miss data know univariate also multiple row know multivariate well look univariate example psychic learn pewter function use impute miss value fairly small data miss value miss value impute strategy mean first calculate mean mean three youll impute mean value miss value data libraries include impute package provide complex ways impute data examples include nearest neighbor soft puke multiple imputation chain equations others thats part section well part three well review work outliers data welcome back module section well look evaluate model success predict result point youve train model time evaluate model determine good predict target future data future instance unknown target value need assess model perform data already know target answer youll assessment proxy performance future data reason hold sample data evaluate test important part phase involve choose appropriate metric business situation think back earlier section problem formulation phase define business problem outcome craft business metric evaluate success model metric choose phase link business metric much possible theres often high correlation metrics addition consider business problem success metric type problem youre work influence model metric choose throughout rest module well look examples common metrics use classification problems also look common metrics use regression problems go start consider simple binary classification problem heres specific example imagine simple image recognition model thats label data either model train test data hold back perform predictions help examine performance model compare predict value actual value plot value table like example start get insights well model perform confusion matrix high level comparison predict class match actual class actual label class identify positive predict label class also true positive good outcome model similarly actual label identify negative predict label class also true negative also good outcome model case model predict correct outcome use test data possible outcomes arent consider good outcomes first actual class predict class positive call false positive prediction positive incorrect finally false negative happen actual class positive predict class negative thats part section well part well review calculate classification metrics welcome back module section well look evaluate model success predict result point youve train model time evaluate model determine good predict target future data future instance unknown target value need assess model perform data already know target answer youll assessment proxy performance future data reason hold sample data evaluate test important part phase involve choose appropriate metric business situation think back earlier section problem formulation phase define business problem outcome craft business metric evaluate success model metric choose phase link business metric much possible theres often high correlation metrics addition consider business problem success metric type problem youre work influence model metric choose throughout rest module well look examples common metrics use classification problems also look common metrics use regression problems go start consider simple binary classification problem heres specific example imagine simple image recognition model thats label data either model train test data hold back perform predictions help examine performance model compare predict value actual value plot value table like example start get insights well model perform confusion matrix high level comparison predict class match actual class actual label class identify positive predict label class also true positive good outcome model similarly actual label identify negative predict label class also true negative also good outcome model case model predict correct outcome use test data possible outcomes arent consider good outcomes first actual class predict class positive call false positive prediction positive incorrect finally false negative happen actual class positive predict class negative thats part section well part well review calculate classification metrics welcome back well continue explore video analysis review create test data final step train model identify test data test data validate evaluate model performance youll perform inference image test data youll compare result label information thats train data create test data alternatively amazon recognition custom label split train data data set use 8020 split split mean data use train use test define train past data set amazon recognition custom label automatically train model service automatically load inspect data select correct machine learn algorithms train model provide model performance metrics charge amount time model take train data contain image label take longer train train complete evaluate performance model test amazon recognition custom label predict test image contain custom label confidence score value quantify certainty model prediction classification problem result map confusion matrix true positive model correctly predict presence custom label test image predict label also grind truth label image example amazon recognition custom label correctly return label present image false positive model incorrectly predict presence custom label test image predict label isnt grind truth label image example amazon recognition custom label return label theres label grind truth image false negative model doesnt predict custom label present image grind truth image include label example amazon recognition custom label doesnt return custom label image contain true negative model correctly predict custom isnt present test image example amazon recognition custom label doesnt return label image doesnt contain console provide access true positive false positive false negative value image test data prediction result use calculate various metrics label aggregate metrics entire test definitions apply predictions model make bound level bound box metrics calculate bound test image regardless whether box prediction grind truth help amazon recognition custom label provide various metrics example view summary metrics evaluation metrics label also advise precision metrics label average precision metric entire test data precision proportion positive result correctly classify amazon recognition custom label provide average recall metrics label average recall metric entire test data recall fraction test label correctly classify use previous example cat would many cat correctly classify service also provide average model performance score label average model performance score entire test data score combine precision recall together give number quantify overall performance particular machine learn algorithm might score class imbalance also want preserve equality precision sensitivity higher value mean better model performance recall precision youre satisfy accuracy model start use thats part three section well part review evaluate improve model welcome back well continue explore video analysis review create test data final step train model identify test data test data validate evaluate model performance youll perform inference image test data youll compare result label information thats train data create test data alternatively amazon recognition custom label split train data data set use 8020 split split mean data use train use test define train past data set amazon recognition custom label automatically train model service automatically load inspect data select correct machine learn algorithms train model provide model performance metrics charge amount time model take train data contain image label take longer train train complete evaluate performance model test amazon recognition custom label predict test image contain custom label confidence score value quantify certainty model prediction classification problem result map confusion matrix true positive model correctly predict presence custom label test image predict label also grind truth label image example amazon recognition custom label correctly return label present image false positive model incorrectly predict presence custom label test image predict label isnt grind truth label image example amazon recognition custom label return label theres label grind truth image false negative model doesnt predict custom label present image grind truth image include label example amazon recognition custom label doesnt return custom label image contain true negative model correctly predict custom isnt present test image example amazon recognition custom label doesnt return label image doesnt contain console provide access true positive false positive false negative value image test data prediction result use calculate various metrics label aggregate metrics entire test definitions apply predictions model make bound level bound box metrics calculate bound test image regardless whether box prediction grind truth help amazon recognition custom label provide various metrics example view summary metrics evaluation metrics label also advise precision metrics label average precision metric entire test data precision proportion positive result correctly classify amazon recognition custom label provide average recall metrics label average recall metric entire test data recall fraction test label correctly classify use previous example cat would many cat correctly classify service also provide average model performance score label average model performance score entire test data score combine precision recall together give number quantify overall performance particular machine learn algorithm might score class imbalance also want preserve equality precision sensitivity higher value mean better model performance recall precision youre satisfy accuracy model start use thats part three section well part review evaluate improve model welcome back review find correlations data quantify linear relationship among variables youre see scatter plot correlation matrix good tool situation convey strong weak linear relationships among numerical variables correlation high minus correlation mean numerical feature perfectly correlate like say proportional correlation variables minus like say proportional minus linear relationship quantify correlation correlation zero mean theres linear relationship doesnt mean theres relationship indication theres linear relationship variables however look number isnt always straightforward often easier view number represent color well look heat highest number dark green minus dark brown color give positive negative directions also show strong correlations seaborn heat function show correlation matrix look chart theres correlation citric acid acidity would expect wine citric acid contribute acidity line however isnt much correlation fix acidity measurement strength acids present acidity measure quantity particular data doesnt appear correlation takeaways section module include point first step data format use easily popular python library work data descriptive statistics help gain insights data visualizations examine data detail thats section well next video welcome back review find correlations data quantify linear relationship among variables youre see scatter plot correlation matrix good tool situation convey strong weak linear relationships among numerical variables correlation high minus correlation mean numerical feature perfectly correlate like say proportional correlation variables minus like say proportional minus linear relationship quantify correlation correlation zero mean theres linear relationship doesnt mean theres relationship indication theres linear relationship variables however look number isnt always straightforward often easier view number represent color well look heat highest number dark green minus dark brown color give positive negative directions also show strong correlations seaborn heat function show correlation matrix look chart theres correlation citric acid acidity would expect wine citric acid contribute acidity line however isnt much correlation fix acidity measurement strength acids present acidity measure quantity particular data doesnt appear correlation takeaways section module include point first step data format use easily popular python library work data descriptive statistics help gain insights data visualizations examine data detail thats section well next video welcome back academy machine learn module great topic today computer vision module well start overview computer vision space youll learn case terminology next well explore detail analyze image video manage service amazon service finally well look customize data set perform object detection module youll able describe case computer vision describe amazon management machine learn service available image video analysis list step require prepare custom data object detection describe amazon sagemaker grind truth use prepare custom data finally amazon recognition perform facial detection thank watch well next video welcome back academy machine learn module great topic today computer vision module well start overview computer vision space youll learn case terminology next well explore detail analyze image video manage service amazon service finally well look customize data set perform object detection module youll able describe case computer vision describe amazon management machine learn service available image video analysis list step require prepare custom data object detection describe amazon sagemaker grind truth use prepare custom data finally amazon recognition perform facial detection thank watch well next video welcome back well continue explore data collection review secure data important consider security data though data set use course public real data customer transactions health record need keep secure identity access management also know service control access resources make sure youre secure data within correctly avoid data breach diagram show simple policy allow read access specific bucket list roll addition control access data need make sure data secure good practice might also legally require certain data type financial data healthcare record provide encryption feature storage service typically data thats rest transit often meet encryption requirements enable encryption object service want protect data transit must secure transport like secure sockets layer transport layer security another aspect consider compliance audit deal data regulate industries youll often need audit access data cloudtrail service enable governance compliance operational audit risk audit account cloud trail continuously monitor retain account activity relate action across entire infrastructure cloud trail provide event history account activity include action take management console sdks command line tool service event history simplify security analysis resource change track troubleshoot also cloudtrail detect unusual activity account feature help simplify operational analysis troubleshoot youre takeaways section look first step solve machine learn problems obtain data require train machine learn model also review use obtain data multiple source service like glue make easy obtain data multiple data store finally make sure understand security requirements base business need regulatory requirements also sure data secure authorize users able access data encrypt possible thats section well next video welcome back well continue explore data collection review secure data important consider security data though data set use course public real data customer transactions health record need keep secure identity access management also know service control access resources make sure youre secure data within correctly avoid data breach diagram show simple policy allow read access specific bucket list roll addition control access data need make sure data secure good practice might also legally require certain data type financial data healthcare record provide encryption feature storage service typically data thats rest transit often meet encryption requirements enable encryption object service want protect data transit must secure transport like secure sockets layer transport layer security another aspect consider compliance audit deal data regulate industries youll often need audit access data cloudtrail service enable governance compliance operational audit risk audit account cloud trail continuously monitor retain account activity relate action across entire infrastructure cloud trail provide event history account activity include action take management console sdks command line tool service event history simplify security analysis resource change track troubleshoot also cloudtrail detect unusual activity account feature help simplify operational analysis troubleshoot youre takeaways section look first step solve machine learn problems obtain data require train machine learn model also review use obtain data multiple source service like glue make easy obtain data multiple data store finally make sure understand security requirements base business need regulatory requirements also sure data secure authorize users able access data encrypt possible thats section well next video welcome amazon academy machine learn foundations module youll learn course objectives various roles machine learn domain learn machine learn complete module able identify course prerequisites objectives indicate role data scientist business identify resources learn go look prerequisites take course take course recommend first complete academy cloud foundations also general technical knowledge include foundational computer literacy skills like basic computer concepts email file management good understand internet also recommend intermediate skills python program general knowledge apply statistics finally general business knowledge important course include insight information technology use business also important business relate skill set communication skills leadership skills orientation towards customer service course youll introduce concepts machine learn tool use youll also introduce work service machine learn youll learn recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods part course youll also learn implement machine learn pipeline include formulate problem business request obtain secure data machine learn build jupiter notebook use amazon sagemaker outline process evaluate data explain data need preprocessed open source tool examine preprocess data also amazon sagemaker train host machine learn model cross validation test performance machine learn model host model inference create amazon sagemaker hyperparameter tune optimize model effectiveness finally manage amazon machine learn service solve specific machine learn problems forecast computer vision natural language process review course outline achieve course objectives youll complete follow modules start module youll introduction machine learn module youll learn implement machine learn pipeline amazon sagemaker modules describe apply manage amazon machine learn service problems forecast computer vision natural language process finally module summary course also include overview step take work towards certify machine learn specialty next five slide provide detail subtopics cover module purpose module introduce major concepts understand machine learn section describe overall field machine learn machine learn relate artificial intelligence deep learn section youll learn common business problems solve machine learn section describe general workflow solve machine learn problems youll also learn common machine learn term section youll review commonly use tool machine learn professionals lastly section youll overview common challenge youll face work machine learn problems module youll introduction amazon sagemaker implement machine learn pipeline module focus application machine learn problems several public domain data set examples machine learn pipeline section introduce define business problems data set module section describe phase machine learn pipeline use computer vision example application section youll learn collect secure data section describe different techniques evaluate data section youll learn process feature engineer section describe step youll take train model sagemaker section youll overview options sagemaker host use model finally section cover evaluate tune model sagemaker module youll introduce use machine learn creep forecast base time series data section youll introduce forecast common applications section outline pitfalls use time series data make forecast finally section youll overview amazon forecast module youll learn use machine learn computer vision section describe general problems solve computer vision section youll learn process analyze image videos section youll learn step youll need take prepare data set computer vision module youll introduce natural language process machine learn section youll learn general problems solve natural language process section review amazon machine learn service address natural language process problems service include amazon transcribe amazon translate amazon amazon comprehend amazon poly module final module course module youll review youve learn throughout course youll also introduce next step take want achieve certify machine learn specialty section module summarize topics youve cover course section youll learn documentation youll also review common frameworks apply service finally section describe step take want continue work towards certify machine learn specialty section youll learn common roles machine learn professionals youre interest data scientist role focus develop analytical statistical program skills data scientist youll skills collect analyze interpret large data set universities offer degrees data science data scientists often degrees relate field like statistics math computer science economics data scientist youll need technical competencies statistics machine learn program languages data analytics youd like career machine learn engineer skills youll need similar data scientist skill like data scientists machine learn engineer also require technical competencies statistics machine learn however youll focus program skills welcome amazon academy machine learn foundations module youll learn course objectives various roles machine learn domain learn machine learn complete module able identify course prerequisites objectives indicate role data scientist business identify resources learn go look prerequisites take course take course recommend first complete academy cloud foundations also general technical knowledge include foundational computer literacy skills like basic computer concepts email file management good understand internet also recommend intermediate skills python program general knowledge apply statistics finally general business knowledge important course include insight information technology use business also important business relate skill set communication skills leadership skills orientation towards customer service course youll introduce concepts machine learn tool use youll also introduce work service machine learn youll learn recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods part course youll also learn implement machine learn pipeline include formulate problem business request obtain secure data machine learn build jupiter notebook use amazon sagemaker outline process evaluate data explain data need preprocessed open source tool examine preprocess data also amazon sagemaker train host machine learn model cross validation test performance machine learn model host model inference create amazon sagemaker hyperparameter tune optimize model effectiveness finally manage amazon machine learn service solve specific machine learn problems forecast computer vision natural language process review course outline achieve course objectives youll complete follow modules start module youll introduction machine learn module youll learn implement machine learn pipeline amazon sagemaker modules describe apply manage amazon machine learn service problems forecast computer vision natural language process finally module summary course also include overview step take work towards certify machine learn specialty next five slide provide detail subtopics cover module purpose module introduce major concepts understand machine learn section describe overall field machine learn machine learn relate artificial intelligence deep learn section youll learn common business problems solve machine learn section describe general workflow solve machine learn problems youll also learn common machine learn term section youll review commonly use tool machine learn professionals lastly section youll overview common challenge youll face work machine learn problems module youll introduction amazon sagemaker implement machine learn pipeline module focus application machine learn problems several public domain data set examples machine learn pipeline section introduce define business problems data set module section describe phase machine learn pipeline use computer vision example application section youll learn collect secure data section describe different techniques evaluate data section youll learn process feature engineer section describe step youll take train model sagemaker section youll overview options sagemaker host use model finally section cover evaluate tune model sagemaker module youll introduce use machine learn creep forecast base time series data section youll introduce forecast common applications section outline pitfalls use time series data make forecast finally section youll overview amazon forecast module youll learn use machine learn computer vision section describe general problems solve computer vision section youll learn process analyze image videos section youll learn step youll need take prepare data set computer vision module youll introduce natural language process machine learn section youll learn general problems solve natural language process section review amazon machine learn service address natural language process problems service include amazon transcribe amazon translate amazon amazon comprehend amazon poly module final module course module youll review youve learn throughout course youll also introduce next step take want achieve certify machine learn specialty section module summarize topics youve cover course section youll learn documentation youll also review common frameworks apply service finally section describe step take want continue work towards certify machine learn specialty section youll learn common roles machine learn professionals youre interest data scientist role focus develop analytical statistical program skills data scientist youll skills collect analyze interpret large data set universities offer degrees data science data scientists often degrees relate field like statistics math computer science economics data scientist youll need technical competencies statistics machine learn program languages data analytics youd like career machine learn engineer skills youll need similar data scientist skill like data scientists machine learn engineer also require technical competencies statistics machine learn however youll focus program skills software architecture analysis interpretation machine learn engineer youll apply program architecture skills design develop machine learn systems machine learn engineer often previous experience software development rely heavily program software engineer machine learn roles might also interest career science apply machine learn technology field machine learn impact everything astronomy zoology many different paths open apply science researcher primary focus type science youre work youll need skills data scientist youll also need know apply skills choose domain thus apply science rule also require technical competencies statistics machine learn many software developers integrate machine learn applications youre interest career software developer also include machine learn technology study machine learn developer primary focus software development skills youll also need skills data scientist make sure take coursework statistics apply mathematics heres final note module recommend review student guide student guide youll find link documentation resources youll throughout course thats introduction thank watch well next video welcome amazon academy machine learn foundations module youll learn course objectives various roles machine learn domain learn machine learn complete module able identify course prerequisites objectives indicate role data scientist business identify resources learn go look prerequisites take course take course recommend first complete academy cloud foundations also general technical knowledge include foundational computer literacy skills like basic computer concepts email file management good understand internet also recommend intermediate skills python program general knowledge apply statistics finally general business knowledge important course include insight information technology use business also important business relate skill set communication skills leadership skills orientation towards customer service course youll introduce concepts machine learn tool use youll also introduce work service machine learn youll learn recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods part course youll also learn implement machine learn pipeline include formulate problem business request obtain secure data machine learn build jupiter notebook use amazon sagemaker outline process evaluate data explain data need preprocessed open source tool examine preprocess data also amazon sagemaker train host machine learn model cross validation test performance machine learn model host model inference create amazon sagemaker hyperparameter tune optimize model effectiveness finally manage amazon machine learn service solve specific machine learn problems forecast computer vision natural language process review course outline achieve course objectives youll complete follow modules start module youll introduction machine learn module youll learn implement machine learn pipeline amazon sagemaker modules describe apply manage amazon machine learn service problems forecast computer vision natural language process finally module summary course also include overview step take work towards certify machine learn specialty next five slide provide detail subtopics cover module purpose module introduce major concepts understand machine learn section describe overall field machine learn machine learn relate artificial intelligence deep learn section youll learn common business problems solve machine learn section describe general workflow solve machine learn problems youll also learn common machine learn term section youll review commonly use tool machine learn professionals lastly section youll overview common challenge youll face work machine learn problems module youll introduction amazon sagemaker implement machine learn pipeline module focus application machine learn problems several public domain data set examples machine learn pipeline section introduce define business problems data set module section describe phase machine learn pipeline use computer vision example application section youll learn collect secure data section describe different techniques evaluate data section youll learn process feature engineer section describe step youll take train model sagemaker section youll overview options sagemaker host use model finally section cover evaluate tune model sagemaker module youll introduce use machine learn creep forecast base time series data section youll introduce forecast common applications section outline pitfalls use time series data make forecast finally section youll overview amazon forecast module youll learn use machine learn computer vision section describe general problems solve computer vision section youll learn process analyze image videos section youll learn step youll need take prepare data set computer vision module youll introduce natural language process machine learn section youll learn general problems solve natural language process section review amazon machine learn service address natural language process problems service include amazon transcribe amazon translate amazon amazon comprehend amazon poly module final module course module youll review youve learn throughout course youll also introduce next step take want achieve certify machine learn specialty section module summarize topics youve cover course section youll learn documentation youll also review common frameworks apply service finally section describe step take want continue work towards certify machine learn specialty section youll learn common roles machine learn professionals youre interest data scientist role focus develop analytical statistical program skills data scientist youll skills collect analyze interpret large data set universities offer degrees data science data scientists often degrees relate field like statistics math computer science economics data scientist youll need technical competencies statistics machine learn program languages data analytics youd like career machine learn engineer skills youll need similar data scientist skill like data scientists machine learn engineer also require technical competencies statistics machine learn however youll focus program skills software architecture analysis interpretation machine learn engineer youll apply program architecture skills design develop machine learn systems machine learn engineer often previous experience software development rely heavily program software engineer machine learn roles might also interest career science apply machine learn technology field machine learn impact everything astronomy zoology many different paths open apply science researcher primary focus type science youre work youll need skills data scientist youll also need know apply skills choose domain thus apply science rule also require technical competencies statistics machine learn many software developers integrate machine learn applications youre interest career software developer also include machine learn technology study machine learn developer primary focus software development skills youll also need skills data scientist make sure take coursework statistics apply mathematics heres final note module recommend review student guide student guide youll find link documentation resources youll throughout course thats introduction thank watch well next video welcome amazon academy machine learn foundations module youll learn course objectives various roles machine learn domain learn machine learn complete module able identify course prerequisites objectives indicate role data scientist business identify resources learn go look prerequisites take course take course recommend first complete academy cloud foundations also general technical knowledge include foundational computer literacy skills like basic computer concepts email file management good understand internet also recommend intermediate skills python program general knowledge apply statistics finally general business knowledge important course include insight information technology use business also important business relate skill set communication skills leadership skills orientation towards customer service course youll introduce concepts machine learn tool use youll also introduce work service machine learn youll learn recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods part course youll also learn implement machine learn pipeline include formulate problem business request obtain secure data machine learn build jupiter notebook use amazon sagemaker outline process evaluate data explain data need preprocessed open source tool examine preprocess data also amazon sagemaker train host machine learn model cross validation test performance machine learn model host model inference create amazon sagemaker hyperparameter tune optimize model effectiveness finally manage amazon machine learn service solve specific machine learn problems forecast computer vision natural language process review course outline achieve course objectives youll complete follow modules start module youll introduction machine learn module youll learn implement machine learn pipeline amazon sagemaker modules describe apply manage amazon machine learn service problems forecast computer vision natural language process finally module summary course also include overview step take work towards certify machine learn specialty next five slide provide detail subtopics cover module purpose module introduce major concepts understand machine learn section describe overall field machine learn machine learn relate artificial intelligence deep learn section youll learn common business problems solve machine learn section describe general workflow solve machine learn problems youll also learn common machine learn term section youll review commonly use tool machine learn professionals lastly section youll overview common challenge youll face work machine learn problems module youll introduction amazon sagemaker implement machine learn pipeline module focus application machine learn problems several public domain data set examples machine learn pipeline section introduce define business problems data set module section describe phase machine learn pipeline use computer vision example application section youll learn collect secure data section describe different techniques evaluate data section youll learn process feature engineer section describe step youll take train model sagemaker section youll overview options sagemaker host use model finally section cover evaluate tune model sagemaker module youll introduce use machine learn creep forecast base time series data section youll introduce forecast common applications section outline pitfalls use time series data make forecast finally section youll overview amazon forecast module youll learn use machine learn computer vision section describe general problems solve computer vision section youll learn process analyze image videos section youll learn step youll need take prepare data set computer vision module youll introduce natural language process machine learn section youll learn general problems solve natural language process section review amazon machine learn service address natural language process problems service include amazon transcribe amazon translate amazon amazon comprehend amazon poly module final module course module youll review youve learn throughout course youll also introduce next step take want achieve certify machine learn specialty section module summarize topics youve cover course section youll learn documentation youll also review common frameworks apply service finally section describe step take want continue work towards certify machine learn specialty section youll learn common roles machine learn professionals youre interest data scientist role focus develop analytical statistical program skills data scientist youll skills collect analyze interpret large data set universities offer degrees data science data scientists often degrees relate field like statistics math computer science economics data scientist youll need technical competencies statistics machine learn program languages data analytics youd like career machine learn engineer skills youll need similar data scientist skill like data scientists machine learn engineer also require technical competencies statistics machine learn however youll focus program skills software architecture analysis interpretation machine learn engineer youll apply program architecture skills design develop machine learn systems machine learn engineer often previous experience software development rely heavily program software engineer machine learn roles might also interest career science apply machine learn technology field machine learn impact everything astronomy zoology many different paths open apply science researcher primary focus type science youre work youll need skills data scientist youll also need know apply skills choose domain thus apply science rule also require technical competencies statistics machine learn many software developers integrate machine learn applications youre interest career software developer also include machine learn technology study machine learn developer primary focus software development skills youll also need skills data scientist make sure take coursework statistics apply mathematics heres final note module recommend review student guide student guide youll find link documentation resources youll throughout course thats introduction thank watch well next video welcome back module section train section go look select model train data preprocessed point youve do clean prepare data doesnt mean data completely ready train algorithm algorithms able work train data data frame format file format like commonly use various algorithms make optimizations file format like record protobuf many amazon sagemaker algorithms support train data format amazon sagemaker require file doesnt header record target variable first column amazon sage maker algorithms work best optimize protobuf record format train data use format allow take advantage pipe mode train algorithms support pipe mode train stream data directly amazon use format target variable train data first column leave feature right target variable column evaluate model data train lead overfitting recall overfitting model learn particulars data well essentially memorize train data rather learn relationships feature label mean model isnt learn relationships pattern listen data future hold split data multiple set commonly set train data validation data test data train data include feature label feed algorithm youve select produce model model make predictions validation data youll likely notice things youll want tweak tune change youre ready test data include feature since want label actually predict performance test data reasonably expect production common split use holdout method use data train validation test data split train validation test small data kfold cross validation utilize much data possible still relatively good metrics order choose model better kfold cross validation randomly partition data different segment segment well rest data outside train order validation particular segment let look example five fold cross validation available train data separate five different chunk train first model use chunk train data go calculate metrics test piece second model go train model train apply test piece thing five time train data test five different model different chunk test data eventually test data point thing note split data data specific order lead bias model especially true youre work structure data example wine data order quality column model test data order pattern apply bias model might also mean target miss train data typically randomize data prior split sufficient many libraries provide function smaller set sometimes useful stratify stratify sample ensure train test set approximately percentage sample target class complete internet search give many ways shuffle split data easiest train test split function sklearn amazon sagemaker provide four different ways train model builtin algorithms available easily deploy console jupiter notebook containers use behind scenes amazon sagemaker build algorithms deal directly amazon sage maker support frameworks provide prebuilt containers support deep learn frameworks apache mxnet tensorflow torch chainer also support machine learn libraries skykit learn sparkml provide prebuilt docker image amazon sagemaker python theyre deploy use respective amazon sagemaker estimator class prebuilt amazon sagemaker container image modify advance scenario package script algorithm amazon sagemaker program language framework develop container example team work build model build containers train host algorithm well someone else already develop tune model worth look marketplace find available model amazon sagemaker provide high performance scalable machine algorithms optimize speed scale accuracy supervise learn amazon sage maker include xgboost linear learner algorithms classification quantitative regression problems also factorization machine address recommendation time series prediction problems amazon sagemaker include support unsupervised learn mean cluster principal component analysis solve problems like identify customer group base purchase behavior finally selection specialize algorithms process image deep learn task let look little closer three commonly use builtin algorithms case xgboost extreme gradient boost popular efficient open source implementation gradient boost tree algorithm gradient boost supervise learn algorithm attempt accurately predict target variable combine ensemble estimate simpler weaker model xgboost do remarkably well machine learn competitions robustly handle variety data type relationships distributions large number hyperparameters tweak tune improve flexibility make xgboost solid choice problems regression classification binary multiclass rank amazon sagemaker linear learner algorithm provide solution classification regression problems amazon sagemaker algorithm simultaneously explore different train objectives choose best solution validation also explore large number model choose best need compare methods provide solution continuous objectives amazon sagemaker linear learner algorithm provide significant increase speed naive hyperparameter optimization techniques mean unsupervised learn algorithm attempt find discrete group within data members group similar possible another different possible members group find attribute want algorithm determine similarity train model amazon sagemaker create train train include amazon bucket store bucket want store output amazon elastic container registry path train code store compute resources want amazon sagemaker model train compute resources compute instance manage amazon sagemaker amazon sagemaker provide selection instance type optimize different machine learn case instance type comprise vary combinations memory network capacity give flexibility choose appropriate resources build train deploy model instance type include instant size allow scale resources requirements target workload takeaways section food split data train test set help validate model accuracy kfold cross validation help smaller data set algorithms supervise learn xgboost linear learner mean unsupervised learn amazon sagemaker train model thats section hope next video welcome back module section train section go look select model train data preprocessed point youve do clean prepare data doesnt mean data completely ready train algorithm algorithms able work train data data frame format file format like commonly use various algorithms make optimizations file format like record protobuf many amazon sagemaker algorithms support train data format amazon sagemaker require file doesnt header record target variable first column amazon sage maker algorithms work best optimize protobuf record format train data use format allow take advantage pipe mode train algorithms support pipe mode train stream data directly amazon use format target variable train data first column leave feature right target variable column evaluate model data train lead overfitting recall overfitting model learn particulars data well essentially memorize train data rather learn relationships feature label mean model isnt learn relationships pattern listen data future hold split data multiple set commonly set train data validation data test data train data include feature label feed algorithm youve select produce model model make predictions validation data youll likely notice things youll want tweak tune change youre ready test data include feature since want label actually predict performance test data reasonably expect production common split use holdout method use data train validation test data split train validation test small data kfold cross validation utilize much data possible still relatively good metrics order choose model better kfold cross validation randomly partition data different segment segment well rest data outside train order validation particular segment let look example five fold cross validation available train data separate five different chunk train first model use chunk train data go calculate metrics test piece second model go train model train apply test piece thing five time train data test five different model different chunk test data eventually test data point thing note split data data specific order lead bias model especially true youre work structure data example wine data order quality column model test data order pattern apply bias model might also mean target miss train data typically randomize data prior split sufficient many libraries provide function smaller set sometimes useful stratify stratify sample ensure train test set approximately percentage sample target class complete internet search give many ways shuffle split data easiest train test split function sklearn amazon sagemaker provide four different ways train model builtin algorithms available easily deploy console jupiter notebook containers use behind scenes amazon sagemaker build algorithms deal directly amazon sage maker support frameworks provide prebuilt containers support deep learn frameworks apache mxnet tensorflow torch chainer also support machine learn libraries skykit learn sparkml provide prebuilt docker image amazon sagemaker python theyre deploy use respective amazon sagemaker estimator class prebuilt amazon sagemaker container image modify advance scenario package script algorithm amazon sagemaker program language framework develop container example team work build model build containers train host algorithm well someone else already develop tune model worth look marketplace find available model amazon sagemaker provide high performance scalable machine algorithms optimize speed scale accuracy supervise learn amazon sage maker include xgboost linear learner algorithms classification quantitative regression problems also factorization machine address recommendation time series prediction problems amazon sagemaker include support unsupervised learn mean cluster principal component analysis solve problems like identify customer group base purchase behavior finally selection specialize algorithms process image deep learn task let look little closer three commonly use builtin algorithms case xgboost extreme gradient boost popular efficient open source implementation gradient boost tree algorithm gradient boost supervise learn algorithm attempt accurately predict target variable combine ensemble estimate simpler weaker model xgboost do remarkably well machine learn competitions robustly handle variety data type relationships distributions large number hyperparameters tweak tune improve flexibility make xgboost solid choice problems regression classification binary multiclass rank amazon sagemaker linear learner algorithm provide solution classification regression problems amazon sagemaker algorithm simultaneously explore different train objectives choose best solution validation also explore large number model choose best need compare methods provide solution continuous objectives amazon sagemaker linear learner algorithm provide significant increase speed naive hyperparameter optimization techniques mean unsupervised learn algorithm attempt find discrete group within data members group similar possible another different possible members group find attribute want algorithm determine similarity train model amazon sagemaker create train train include amazon bucket store bucket want store output amazon elastic container registry path train code store compute resources want amazon sagemaker model train compute resources compute instance manage amazon sagemaker amazon sagemaker provide selection instance type optimize different machine learn case instance type comprise vary combinations memory network capacity give flexibility choose appropriate resources build train deploy model instance type include instant size allow scale resources requirements target workload takeaways section food split data train test set help validate model accuracy kfold cross validation help smaller data set algorithms supervise learn xgboost linear learner mean unsupervised learn amazon sagemaker train model thats section hope next video welcome back module section train section go look select model train data preprocessed point youve do clean prepare data doesnt mean data completely ready train algorithm algorithms able work train data data frame format file format like commonly use various algorithms make optimizations file format like record protobuf many amazon sagemaker algorithms support train data format amazon sagemaker require file doesnt header record target variable first column amazon sage maker algorithms work best optimize protobuf record format train data use format allow take advantage pipe mode train algorithms support pipe mode train stream data directly amazon use format target variable train data first column leave feature right target variable column evaluate model data train lead overfitting recall overfitting model learn particulars data well essentially memorize train data rather learn relationships feature label mean model isnt learn relationships pattern listen data future hold split data multiple set commonly set train data validation data test data train data include feature label feed algorithm youve select produce model model make predictions validation data youll likely notice things youll want tweak tune change youre ready test data include feature since want label actually predict performance test data reasonably expect production common split use holdout method use data train validation test data split train validation test small data kfold cross validation utilize much data possible still relatively good metrics order choose model better kfold cross validation randomly partition data different segment segment well rest data outside train order validation particular segment let look example five fold cross validation available train data separate five different chunk train first model use chunk train data go calculate metrics test piece second model go train model train apply test piece thing five time train data test five different model different chunk test data eventually test data point thing note split data data specific order lead bias model especially true youre work structure data example wine data order quality column model test data order pattern apply bias model might also mean target miss train data typically randomize data prior split sufficient many libraries provide function smaller set sometimes useful stratify stratify sample ensure train test set approximately percentage sample target class complete internet search give many ways shuffle split data easiest train test split function sklearn amazon sagemaker provide four different ways train model builtin algorithms available easily deploy console jupiter notebook containers use behind scenes amazon sagemaker build algorithms deal directly amazon sage maker support frameworks provide prebuilt containers support deep learn frameworks apache mxnet tensorflow torch chainer also support machine learn libraries skykit learn sparkml provide prebuilt docker image amazon sagemaker python theyre deploy use respective amazon sagemaker estimator class prebuilt amazon sagemaker container image modify advance scenario package script algorithm amazon sagemaker program language framework develop container example team work build model build containers train host algorithm well someone else already develop tune model worth look marketplace find available model amazon sagemaker provide high performance scalable machine algorithms optimize speed scale accuracy supervise learn amazon sage maker include xgboost linear learner algorithms classification quantitative regression problems also factorization machine address recommendation time series prediction problems amazon sagemaker include support unsupervised learn mean cluster principal component analysis solve problems like identify customer group base purchase behavior finally selection specialize algorithms process image deep learn task let look little closer three commonly use builtin algorithms case xgboost extreme gradient boost popular efficient open source implementation gradient boost tree algorithm gradient boost supervise learn algorithm attempt accurately predict target variable combine ensemble estimate simpler weaker model xgboost do remarkably well machine learn competitions robustly handle variety data type relationships distributions large number hyperparameters tweak tune improve flexibility make xgboost solid choice problems regression classification binary multiclass rank amazon sagemaker linear learner algorithm provide solution classification regression problems amazon sagemaker algorithm simultaneously explore different train objectives choose best solution validation also explore large number model choose best need compare methods provide solution continuous objectives amazon sagemaker linear learner algorithm provide significant increase speed naive hyperparameter optimization techniques mean unsupervised learn algorithm attempt find discrete group within data members group similar possible another different possible members group find attribute want algorithm determine similarity train model amazon sagemaker create train train include amazon bucket store bucket want store output amazon elastic container registry path train code store compute resources want amazon sagemaker model train compute resources compute instance manage amazon sagemaker amazon sagemaker provide selection instance type optimize different machine learn case instance type comprise vary combinations memory network capacity give flexibility choose appropriate resources build train deploy model instance type include instant size allow scale resources requirements target workload takeaways section food split data train test set help validate model accuracy kfold cross validation help smaller data set algorithms supervise learn xgboost linear learner mean unsupervised learn amazon sagemaker train model thats section hope next video welcome back module section train section go look select model train data preprocessed point youve do clean prepare data doesnt mean data completely ready train algorithm algorithms able work train data data frame format file format like commonly use various algorithms make optimizations file format like record protobuf many amazon sagemaker algorithms support train data format amazon sagemaker require file doesnt header record target variable first column amazon sage maker algorithms work best optimize protobuf record format train data use format allow take advantage pipe mode train algorithms support pipe mode train stream data directly amazon use format target variable train data first column leave feature right target variable column evaluate model data train lead overfitting recall overfitting model learn particulars data well essentially memorize train data rather learn relationships feature label mean model isnt learn relationships pattern listen data future hold split data multiple set commonly set train data validation data test data train data include feature label feed algorithm youve select produce model model make predictions validation data youll likely notice things youll want tweak tune change youre ready test data include feature since want label actually predict performance test data reasonably expect production common split use holdout method use data train validation test data split train validation test small data kfold cross validation utilize much data possible still relatively good metrics order choose model better kfold cross validation randomly partition data different segment segment well rest data outside train order validation particular segment let look example five fold cross validation available train data separate five different chunk train first model use chunk train data go calculate metrics test piece second model go train model train apply test piece thing five time train data test five different model different chunk test data eventually test data point thing note split data data specific order lead bias model especially true youre work structure data example wine data order quality column model test data order pattern apply bias model might also mean target miss train data typically randomize data prior split sufficient many libraries provide function smaller set sometimes useful stratify stratify sample ensure train test set approximately percentage sample target class complete internet search give many ways shuffle split data easiest train test split function sklearn amazon sagemaker provide four different ways train model builtin algorithms available easily deploy console jupiter notebook containers use behind scenes amazon sagemaker build algorithms deal directly amazon sage maker support frameworks provide prebuilt containers support deep learn frameworks apache mxnet tensorflow torch chainer also support machine learn libraries skykit learn sparkml provide prebuilt docker image amazon sagemaker python theyre deploy use respective amazon sagemaker estimator class prebuilt amazon sagemaker container image modify advance scenario package script algorithm amazon sagemaker program language framework develop container example team work build model build containers train host algorithm well someone else already develop tune model worth look marketplace find available model amazon sagemaker provide high performance scalable machine algorithms optimize speed scale accuracy supervise learn amazon sage maker include xgboost linear learner algorithms classification quantitative regression problems also factorization machine address recommendation time series prediction problems amazon sagemaker include support unsupervised learn mean cluster principal component analysis solve problems like identify customer group base purchase behavior finally selection specialize algorithms process image deep learn task let look little closer three commonly use builtin algorithms case xgboost extreme gradient boost popular efficient open source implementation gradient boost tree algorithm gradient boost supervise learn algorithm attempt accurately predict target variable combine ensemble estimate simpler weaker model xgboost do remarkably well machine learn competitions robustly handle variety data type relationships distributions large number hyperparameters tweak tune improve flexibility make xgboost solid choice problems regression classification binary multiclass rank amazon sagemaker linear learner algorithm provide solution classification regression problems amazon sagemaker algorithm simultaneously explore different train objectives choose best solution validation also explore large number model choose best need compare methods provide solution continuous objectives amazon sagemaker linear learner algorithm provide significant increase speed naive hyperparameter optimization techniques mean unsupervised learn algorithm attempt find discrete group within data members group similar possible another different possible members group find attribute want algorithm determine similarity train model amazon sagemaker create train train include amazon bucket store bucket want store output amazon elastic container registry path train code store compute resources want amazon sagemaker model train compute resources compute instance manage amazon sagemaker amazon sagemaker provide selection instance type optimize different machine learn case instance type comprise vary combinations memory network capacity give flexibility choose appropriate resources build train deploy model instance type include instant size allow scale resources requirements target workload takeaways section food split data train test set help validate model accuracy kfold cross validation help smaller data set algorithms supervise learn xgboost linear learner mean unsupervised learn amazon sagemaker train model thats section hope next video welcome back time review module wrap module learn describe business problem solve amazon forecast describe challenge work time series data list step require create forecast use amazon forecast amazon forecast make prediction thank participate next module welcome back time review module wrap module learn describe business problem solve amazon forecast describe challenge work time series data list step require create forecast use amazon forecast amazon forecast make prediction thank participate next module welcome back well continue explore data collection review extract transform load data data typically spread across many different systems data providers present challenge youll need bring data source together something consume machine learn model extract transform load also know step define extract step pull data source single location extraction might need modify data combine match record task transform data finally load step data load repository amazon typical framework components example consider diagram first crawler program connect data store source target progress rank list classifiers determine schema data create metadata table glue data catalog define business logic thats need perform work youll need schedule event final note service discuss exist transform partition process glue fully manage service make simple costeffective categorize data clean reach move reliably various data store glue consist central metadata repository know glue data engine automatically generate python scala code also provide flexible scheduler handle dependency resolution monitor retry glue serverless dont need manage infrastructure glue console discover data transform make available search query console call underlie service orchestrate work need transform data also glue operations interface glue service edit debug test python scala apache spark code use familiar development environment glue well suit machine learn receive label data use train heres example provide glue train data teach model duplicate record data source look like glue identify duplicate present analysis data engineer glue enable orchestration complex job example glue crawl data source present information clients data catalog glue job base event get data example lambda function trigger job soon data become available amazon also register data glue data catalog part job although manage tool available manipulate data data scientist also write script jupiter notebook handle data simple extract load script show import variable section import libraries use note bodo library variables also file location local folder extraction download extract section make request save bite stream stream pass file function use extract data extract file folder upload section enumerate folders file upload file amazon discover script use often migrate standalone function import python applications thats part section part review secure data welcome back well continue explore data collection review extract transform load data data typically spread across many different systems data providers present challenge youll need bring data source together something consume machine learn model extract transform load also know step define extract step pull data source single location extraction might need modify data combine match record task transform data finally load step data load repository amazon typical framework components example consider diagram first crawler program connect data store source target progress rank list classifiers determine schema data create metadata table glue data catalog define business logic thats need perform work youll need schedule event final note service discuss exist transform partition process glue fully manage service make simple costeffective categorize data clean reach move reliably various data store glue consist central metadata repository know glue data engine automatically generate python scala code also provide flexible scheduler handle dependency resolution monitor retry glue serverless dont need manage infrastructure glue console discover data transform make available search query console call underlie service orchestrate work need transform data also glue operations interface glue service edit debug test python scala apache spark code use familiar development environment glue well suit machine learn receive label data use train heres example provide glue train data teach model duplicate record data source look like glue identify duplicate present analysis data engineer glue enable orchestration complex job example glue crawl data source present information clients data catalog glue job base event get data example lambda function trigger job soon data become available amazon also register data glue data catalog part job although manage tool available manipulate data data scientist also write script jupiter notebook handle data simple extract load script show import variable section import libraries use note bodo library variables also file location local folder extraction download extract section make request save bite stream stream pass file function use extract data extract file folder upload section enumerate folders file upload file amazon discover script use often migrate standalone function import python applications thats part section part review secure data welcome back module section go take look data set well module well also look guidance formulate business problem start heres reminder machine learn pipeline look previous module map section module section section cover formulate problem also cover data set well throughout module section well discuss obtain secure data machine learn activities section show tool techniques gain understand data section well look preprocessing data ready train model section cover select train appropriate machine learn model section show deploy model make prediction section examine process evaluate performance machine learn model finally section well look tune model machine learn pipeline iterative process work real world problem might find iterate many time arrive solution meet businesss need first section well examine think turn business requirement machine learn problem first step phase simply define problem want solve goal want reach understand business goal youll measure solution unusual solidify business problem begin target solution question develop good understand problem information problem begin frame approach first problem even solve machine learn would traditional approach make sense supervise unsupervised machine learn problem label data train supervise model many question could business ultimately validate machine learn make sure access right people data also come simplest solution problem heres example want identify fraudulent credit card transactions stop transaction process thats problem whats business goal outcome drive problem statement case intend outcome reduction number customers membership credit card result fraudulent transaction business perspective define success give problem desire outcome stage need move qualitative statements quantitative statements easily measure continue example metric could define success problem might reduction number customers file claim fraudulent transactions within 6month period youve define business side problem time start think term machine learn model whats actual output want model want specific statement reflect model could actually output example might model output whether credit card transaction fraudulent fraudulent know want model actually achieve information determine type youre work historical data customers file report fraud transactions data machine learn purpose historical data fall supervise learn approach label already define recall earlier course supervise type categorize group classification regression credit card example desire output categorize transaction fraud fraud youre deal binary classification problem throughout module youll several data set use access data set many irvine machine learn repository first data contain numerical information composition wine along quality line question might want data base composition wine could predict quality therefore price addition question well also data view statistics deal outliers scale numerical data second data evaluation database data heavily textbased enable explore encode categorical data convert text value number process machine learn third data biomedical data also labs question answer data base biomechanical feature predict patient abnormality data take entire endtoend process youll train model thats tune make prediction section look business problems need convert problem also look question define success measure outcome impact solution implement business problems fall categories first category classification binary multiclass target belong class second category regression predict numerical value thats section well next video welcome back module section go take look data set well module well also look guidance formulate business problem start heres reminder machine learn pipeline look previous module map section module section section cover formulate problem also cover data set well throughout module section well discuss obtain secure data machine learn activities section show tool techniques gain understand data section well look preprocessing data ready train model section cover select train appropriate machine learn model section show deploy model make prediction section examine process evaluate performance machine learn model finally section well look tune model machine learn pipeline iterative process work real world problem might find iterate many time arrive solution meet businesss need first section well examine think turn business requirement machine learn problem first step phase simply define problem want solve goal want reach understand business goal youll measure solution unusual solidify business problem begin target solution question develop good understand problem information problem begin frame approach first problem even solve machine learn would traditional approach make sense supervise unsupervised machine learn problem label data train supervise model many question could business ultimately validate machine learn make sure access right people data also come simplest solution problem heres example want identify fraudulent credit card transactions stop transaction process thats problem whats business goal outcome drive problem statement case intend outcome reduction number customers membership credit card result fraudulent transaction business perspective define success give problem desire outcome stage need move qualitative statements quantitative statements easily measure continue example metric could define success problem might reduction number customers file claim fraudulent transactions within 6month period youve define business side problem time start think term machine learn model whats actual output want model want specific statement reflect model could actually output example might model output whether credit card transaction fraudulent fraudulent know want model actually achieve information determine type youre work historical data customers file report fraud transactions data machine learn purpose historical data fall supervise learn approach label already define recall earlier course supervise type categorize group classification regression credit card example desire output categorize transaction fraud fraud youre deal binary classification problem throughout module youll several data set use access data set many irvine machine learn repository first data contain numerical information composition wine along quality line question might want data base composition wine could predict quality therefore price addition question well also data view statistics deal outliers scale numerical data second data evaluation database data heavily textbased enable explore encode categorical data convert text value number process machine learn third data biomedical data also labs question answer data base biomechanical feature predict patient abnormality data take entire endtoend process youll train model thats tune make prediction section look business problems need convert problem also look question define success measure outcome impact solution implement business problems fall categories first category classification binary multiclass target belong class second category regression predict numerical value thats section well next video welcome back section well look tool youll use throughout rest course start list isnt exhaustive list tool available today go cover high level good place start first theres jupiter notebook jupiter notebook isnt open source application create share document contain live code equations visualizations narrative text use include data clean transformation numerical simulation statistical model data visualization machine learn much jupiter webbased interactive development environment jupiter notebooks code data jupiter flexible configure arrange user interface support wide range workflows data science scientific compute machine learn jupiter extensible modular write plugins components integrate exist ones later course youll amazon sage maker host jupiter notebooks jupiter pandas open source python library use data handle analysis pandas represent data table similar spreadsheet table know pandas dataframe matplotlib python library create scientific static animate interactive visualizations python youll generate plot data later course seaborn another data visualization library python thats build matplotlib provide high level interface draw attractive informative statistical numpy fundamental scientific compute package python contain function dimensional array object also useful math function linear algebra transform random number capabilities sidekick learn open source machine learn library support supervise unsupervised learn also provide various tool model fit data preprocessing model selection evaluation many utilities scientific learn build numpy matplot live good tool explore machine learn although youll borrow function course might want consider explore complete score move individual libraries package also tool contain production ready frameworks already mention sidekit learn good library machine learn framework support tensorflow keras also include libraries machine learn frameworks list support use amazon sagemaker also provide compute instance tune machine learn cloud edge compute instance optimize learn inference another resource certain amazon machine image amis offer prepackaged amis contain many popular frameworks finally theres amazon sage maker service many capabilities first sagemaker deploy machine learn instance run jupiter notebooks jupiter manage deployment resources need connect jupiter environment sagemaker also provide tool label data train model host train model marketplace also provide selection ready model package algorithms thirdparty machine learn developers also provide management machine learn service integrate applications even dont substantial machine learn experience computer vision amazon recognition provide object facial recognition image video also amazon text tract extract text image speech service include amazon poly speak text another speech service amazon transcribe convert speak audio text language amazon comprehend use find insights relationships text also amazon translate translate text different languages want work chatbots amazon help build interactive conversational applications voice text forecast amazon forecast use machine learn combine time series data additional variables build forecast finally youd like work recommendations amazon personalize help create individual personalize recommendations customers manage service already train many aspects problem domain need provide specific data start go look many manage service second half course learn things takeaways section include point first python popular language perform machine learn task jupiter book provide webbased host development environment machine learn youll jupiter notebooks frequently machine learn large number open source tool pandas youll often machine learn practitioner finally depend upon requirements might start lowlevel frameworks create solution might also tool amazon sagemaker help heavy lift could simply adapt manage amazon service specific problem domain thats video well next welcome back section well look tool youll use throughout rest course start list isnt exhaustive list tool available today go cover high level good place start first theres jupiter notebook jupiter notebook isnt open source application create share document contain live code equations visualizations narrative text use include data clean transformation numerical simulation statistical model data visualization machine learn much jupiter webbased interactive development environment jupiter notebooks code data jupiter flexible configure arrange user interface support wide range workflows data science scientific compute machine learn jupiter extensible modular write plugins components integrate exist ones later course youll amazon sage maker host jupiter notebooks jupiter pandas open source python library use data handle analysis pandas represent data table similar spreadsheet table know pandas dataframe matplotlib python library create scientific static animate interactive visualizations python youll generate plot data later course seaborn another data visualization library python thats build matplotlib provide high level interface draw attractive informative statistical numpy fundamental scientific compute package python contain function dimensional array object also useful math function linear algebra transform random number capabilities sidekick learn open source machine learn library support supervise unsupervised learn also provide various tool model fit data preprocessing model selection evaluation many utilities scientific learn build numpy matplot live good tool explore machine learn although youll borrow function course might want consider explore complete score move individual libraries package also tool contain production ready frameworks already mention sidekit learn good library machine learn framework support tensorflow keras also include libraries machine learn frameworks list support use amazon sagemaker also provide compute instance tune machine learn cloud edge compute instance optimize learn inference another resource certain amazon machine image amis offer prepackaged amis contain many popular frameworks finally theres amazon sage maker service many capabilities first sagemaker deploy machine learn instance run jupiter notebooks jupiter manage deployment resources need connect jupiter environment sagemaker also provide tool label data train model host train model marketplace also provide selection ready model package algorithms thirdparty machine learn developers also provide management machine learn service integrate applications even dont substantial machine learn experience computer vision amazon recognition provide object facial recognition image video also amazon text tract extract text image speech service include amazon poly speak text another speech service amazon transcribe convert speak audio text language amazon comprehend use find insights relationships text also amazon translate translate text different languages want work chatbots amazon help build interactive conversational applications voice text forecast amazon forecast use machine learn combine time series data additional variables build forecast finally youd like work recommendations amazon personalize help create individual personalize recommendations customers manage service already train many aspects problem domain need provide specific data start go look many manage service second half course learn things takeaways section include point first python popular language perform machine learn task jupiter book provide webbased host development environment machine learn youll jupiter notebooks frequently machine learn large number open source tool pandas youll often machine learn practitioner finally depend upon requirements might start lowlevel frameworks create solution might also tool amazon sagemaker help heavy lift could simply adapt manage amazon service specific problem domain thats video well next well start review natural language process mean natural language process also know explain consider example amazon alexa alexa work device amazon echo record word record speech send amazon servers analyze efficiently amazon break phrase individual sound connect database contain pronunciation various word find word closely correspond combination individual sound amazon identify important word make sense task carry correspond function instance alexa notice word like outside temperature open weather alexa skill amazon servers send information back device alexa speak broad term general business computational problems solve machine learn however systems predate machine learn example speech text older smartphone cell phone use screen readers many systems form machine learn consider hierarchical structure language word lowest layer hierarchy group word make phrase next level phrase make sentence ultimately sentence convey ideas systems face several significant challenge well look challenge next language isnt word different mean base word surround know context often word phrase multiple mean example consider term weather could weather colloquial mean english youre sick could theres wonderful weather outside mean weather condition outside good phrase really could convey surprise disagreement many things depend context inflection main challenge challenge discover structure text first task application break text meaningful units word phrase sentence another challenge label data system convert data must apply label represent various part speech every language require different label scheme match languages grammar also face challenge represent context word mean depend heavily context system need represent large challenge many contexts difficult convert context form computers understand finally although grammar define structure language application grammar indescribably large scope handle variation languages use humans major challenge systems thats machine learn large impact apply range problems common applications include search applications google human machine interactions like alexa sentiment analysis market political campaign social research base media analysis chatbots mimic human speech applications apply machine learn development pipeline youve see throughout course develop solution first task formulate problem collect label data collect data consist break text meaningful subsets label set feature engineer major part applications process get complicate youre deal highly irregular unstructured text example youre build application classify document need able distinguish word common term different mean label data domain sometimes also call tag label process assign individual text string different part speech specialize tool label first task application convert text data analyze convert text remove word arent need analysis input text example word remove leave phrase sample text remove stop word normalize text convert similar word common form example word run different form word normalize instance word block text use stem limitation process limitation group different form word single term limitation versions word would group instance form single term stem hand remove character stem algorithm consider unnecessary stem might work example form might recognize form word youve normalize text standardize remove word arent dictionary youre use analysis example could remove acronyms slang special character natural language toolkit also know nltk python library provide function remove stop word normalize text another first step create system convert text data collection data frame libraries provide function assist process example show use word tokenize function nltk library youve clean text load data frame apply model create feature couple common model first model know word simple model capture frequency word document model create word value number time word occur document second model term frequency inverse document frequency also know term frequency count many time word appear document inverse document frequency number time word occur document value use together calculate weight word word frequently appear many document lower weight many establish model field example show word model word vector model vector model convert sentence phrase vector mathematical object record directionality magnitude example simple sentence convert vector frequency word record word value appear twice sentence word often use classify document different categories also use derive attribute fee applications sentiment analysis three broad picture text analysis first classification text similar classification systems youve see course text provide input process extract feature send feature machine learn algorithm interact classifier model infer classification many applications text match example autocorrect spell grammar check base text match algorithm edit distance also know leavenstein distance frequently use derive relationships different word phrase text use process call coreference resolution several systems provide python libraries derive relationships biggest challenge describe context text consider example user search term tablet word tablet least distinct mean search engine need know mean user mind search engines rely commonly use context term isnt qualify example add another term like medicine compute search process extract entities know name entity recognition model follow function first identify noun phrase use dependency chart part speech tag classify phrase use classification algorithm word finally disambiguate entities use knowledge graph heres example use extract entities titanic north atlantic text name entities extract knowledge graph extract mean knowledge graph combine subject matter expertise machine learn drive mean amazon recommendations engine example knowledge graph main point remember section first predate machine learn workflow youve see modules main case search query analysis human machine interaction market social research complicate human language lack precision thank watch well next video well start review natural language process mean natural language process also know explain consider example amazon alexa alexa work device amazon echo record word record speech send amazon servers analyze efficiently amazon break phrase individual sound connect database contain pronunciation various word find word closely correspond combination individual sound amazon identify important word make sense task carry correspond function instance alexa notice word like outside temperature open weather alexa skill amazon servers send information back device alexa speak broad term general business computational problems solve machine learn however systems predate machine learn example speech text older smartphone cell phone use screen readers many systems form machine learn consider hierarchical structure language word lowest layer hierarchy group word make phrase next level phrase make sentence ultimately sentence convey ideas systems face several significant challenge well look challenge next language isnt word different mean base word surround know context often word phrase multiple mean example consider term weather could weather colloquial mean english youre sick could theres wonderful weather outside mean weather condition outside good phrase really could convey surprise disagreement many things depend context inflection main challenge challenge discover structure text first task application break text meaningful units word phrase sentence another challenge label data system convert data must apply label represent various part speech every language require different label scheme match languages grammar also face challenge represent context word mean depend heavily context system need represent large challenge many contexts difficult convert context form computers understand finally although grammar define structure language application grammar indescribably large scope handle variation languages use humans major challenge systems thats machine learn large impact apply range problems common applications include search applications google human machine interactions like alexa sentiment analysis market political campaign social research base media analysis chatbots mimic human speech applications apply machine learn development pipeline youve see throughout course develop solution first task formulate problem collect label data collect data consist break text meaningful subsets label set feature engineer major part applications process get complicate youre deal highly irregular unstructured text example youre build application classify document need able distinguish word common term different mean label data domain sometimes also call tag label process assign individual text string different part speech specialize tool label first task application convert text data analyze convert text remove word arent need analysis input text example word remove leave phrase sample text remove stop word normalize text convert similar word common form example word run different form word normalize instance word block text use stem limitation process limitation group different form word single term limitation versions word would group instance form single term stem hand remove character stem algorithm consider unnecessary stem might work example form might recognize form word youve normalize text standardize remove word arent dictionary youre use analysis example could remove acronyms slang special character natural language toolkit also know nltk python library provide function remove stop word normalize text another first step create system convert text data collection data frame libraries provide function assist process example show use word tokenize function nltk library youve clean text load data frame apply model create feature couple common model first model know word simple model capture frequency word document model create word value number time word occur document second model term frequency inverse document frequency also know term frequency count many time word appear document inverse document frequency number time word occur document value use together calculate weight word word frequently appear many document lower weight many establish model field example show word model word vector model vector model convert sentence phrase vector mathematical object record directionality magnitude example simple sentence convert vector frequency word record word value appear twice sentence word often use classify document different categories also use derive attribute fee applications sentiment analysis three broad picture text analysis first classification text similar classification systems youve see course text provide input process extract feature send feature machine learn algorithm interact classifier model infer classification many applications text match example autocorrect spell grammar check base text match algorithm edit distance also know leavenstein distance frequently use derive relationships different word phrase text use process call coreference resolution several systems provide python libraries derive relationships biggest challenge describe context text consider example user search term tablet word tablet least distinct mean search engine need know mean user mind search engines rely commonly use context term isnt qualify example add another term like medicine compute search process extract entities know name entity recognition model follow function first identify noun phrase use dependency chart part speech tag classify phrase use classification algorithm word finally disambiguate entities use knowledge graph heres example use extract entities titanic north atlantic text name entities extract knowledge graph extract mean knowledge graph combine subject matter expertise machine learn drive mean amazon recommendations engine example knowledge graph main point remember section first predate machine learn workflow youve see modules main case search query analysis human machine interaction market social research complicate human language lack precision thank watch well next video welcome back time review module wrap summary module learn describe case solve use manage amazon service describe manage service available good thank watch well next module welcome back time review module wrap summary module learn describe case solve use manage amazon service describe manage service available good thank watch well next module welcome section section go talk machine learn course introduction machine learn also know first well discuss machine learn fit larger picture machine learn subset artificial intelligence broad branch computer science thats focus build machine human task deep learn subdomain machine learn understand together well discuss mention machine learn subset broader computer science field know artificial intelligence focus build machine perform task human would typically perform contemporary popular culture youve probably see movies television work fiction example might see control world around start act initiative start computer agents perceive environments take action achieve specific goal though maybe outcome creators originally wish fictional interact extensively humans helpers workers generally better work humanity theyre general purpose kinds examples artificial general intelligence capacity learn understand task human problems typically span many field research natural language process reason knowledge representation learn perception physical environment interaction isnt reality unless truly live simulation every year move closer areas might also read see commentary ethics create view positive perhaps partly fear malicious fictional want destroy humanity power source perhaps theyre concern risk mass unemployment intelligent machine could work need break dont worry though go build next rogue course maybe next search youll probably find many definitions machine learn isnt universally agree upon definition well start look couple definitions example could machine learn scientific study algorithms statistical model perform task use inference instead instructions isnt point use algorithms statistical model instead instructions help better understand apply idea concrete example suppose need write application determine email message spam without machine learn need write complex series decision statement use else statements youd also need word subject body number link length message determine email message spam would hard labor intensive build large rule cover every possibility machine learn however could list email message mark spam spam train machine learn model model would learn pattern word length attribute good indicators spam message present model email message hadnt model would perform prediction whether message spam spam deep learn represent significant leap forward capabilities artificial intelligence machine learn theory behind deep learn create human brain work artificial neural network inspire biological neurons find brain although implementation become different artificial neurons input single output neurons fire activate output base transformation input neural network compose layer artificial neurons connections layer typically inputoutput hide layer network output single neuron connect input neurons next layer work ask solve problem input layer populate train data neurons activate throughout layer answer present output layer accuracy output measure output doesnt meet threshold train repeat slight change weight connections neurons neural network repeatedly time strengthen connections lead success diminish connections lead failure youll course machine learn practitioners spend time optimize model select best data feature train select model best result contrast deep learn practitioners spend almost time task instead spend time model data different architectures theory deep learn go back decades hardware need deep learn problems wasnt generally accessible recently available deep learn address problems complex problems could work mainstream machine learn recent occurrence rapid advancements machine deep learn start around mid2000s partly moor rise cloud compute result easier access larger faster cheaper compute storage capabilities rent compute power hours pennys need substantial investments operate large scale compute cluster 2012 neural network start use imagenet large scale visual recognition challenge machine learn competition image recognition accuracy rate jump steadily climb ever since fact exceed human performance 2015 takeaways section first artificial intelligence broad field build machine perform human task also machine learn subset focus use data train machine learn model make predictions deep learn technique inspire human biology use layer artificial neurons build network solve problems last advancements technology cloud compute algorithm development correspond advance machine learn capabilities applications thats section well next video welcome section section go talk machine learn course introduction machine learn also know first well discuss machine learn fit larger picture machine learn subset artificial intelligence broad branch computer science thats focus build machine human task deep learn subdomain machine learn understand together well discuss mention machine learn subset broader computer science field know artificial intelligence focus build machine perform task human would typically perform contemporary popular culture youve probably see movies television work fiction example might see control world around start act initiative start computer agents perceive environments take action achieve specific goal though maybe outcome creators originally wish fictional interact extensively humans helpers workers generally better work humanity theyre general purpose kinds examples artificial general intelligence capacity learn understand task human problems typically span many field research natural language process reason knowledge representation learn perception physical environment interaction isnt reality unless truly live simulation every year move closer areas might also read see commentary ethics create view positive perhaps partly fear malicious fictional want destroy humanity power source perhaps theyre concern risk mass unemployment intelligent machine could work need break dont worry though go build next rogue course maybe next search youll probably find many definitions machine learn isnt universally agree upon definition well start look couple definitions example could machine learn scientific study algorithms statistical model perform task use inference instead instructions isnt point use algorithms statistical model instead instructions help better understand apply idea concrete example suppose need write application determine email message spam without machine learn need write complex series decision statement use else statements youd also need word subject body number link length message determine email message spam would hard labor intensive build large rule cover every possibility machine learn however could list email message mark spam spam train machine learn model model would learn pattern word length attribute good indicators spam message present model email message hadnt model would perform prediction whether message spam spam deep learn represent significant leap forward capabilities artificial intelligence machine learn theory behind deep learn create human brain work artificial neural network inspire biological neurons find brain although implementation become different artificial neurons input single output neurons fire activate output base transformation input neural network compose layer artificial neurons connections layer typically inputoutput hide layer network output single neuron connect input neurons next layer work ask solve problem input layer populate train data neurons activate throughout layer answer present output layer accuracy output measure output doesnt meet threshold train repeat slight change weight connections neurons neural network repeatedly time strengthen connections lead success diminish connections lead failure youll course machine learn practitioners spend time optimize model select best data feature train select model best result contrast deep learn practitioners spend almost time task instead spend time model data different architectures theory deep learn go back decades hardware need deep learn problems wasnt generally accessible recently available deep learn address problems complex problems could work mainstream machine learn recent occurrence rapid advancements machine deep learn start around mid2000s partly moor rise cloud compute result easier access larger faster cheaper compute storage capabilities rent compute power hours pennys need substantial investments operate large scale compute cluster 2012 neural network start use imagenet large scale visual recognition challenge machine learn competition image recognition accuracy rate jump steadily climb ever since fact exceed human performance 2015 takeaways section first artificial intelligence broad field build machine perform human task also machine learn subset focus use data train machine learn model make predictions deep learn technique inspire human biology use layer artificial neurons build network solve problems last advancements technology cloud compute algorithm development correspond advance machine learn capabilities applications thats section well next video welcome back section well review five management machine learn service various case service simplify process create machine learn application start look amazon transcribe amazon transcribe recognize speech audio file produce transcription recognize specific voice audio file create customize vocabulary term specialize particular domain also transcription service applications integrate websockets internet protocol twoway communication application amazon transcribe common case amazon transcribe first medical professionals record note amazon transcribe capture speak note text also video production organizations generate subtitle automatically video could also do real time live fee close caption media company amazon transcribe capture label content fee content amazon comprehend analysis last company record customer service sales call transcribe analyze result train strategic opportunities amazon poly convert text lifelike speech input either plain text file file thats format speech synthesis markup language ssml ssml markup language use provide special instructions speech sound example introduce pause flow speech ssml instruct amazon poly pause word also output speech amazon poly vorbis audio stream format amazon poly eligible certain regulate workloads example eligible health insurance portability accountability 1996 hipaa amazon poly also eligible payment card industry data security standard common case amazon poly first example major news company use amazon poly generate vocal content directly write stories also embed map apis developers voice geob vacation language train company use amazon poly create systems learn language finally animators use voice character amazon translate create multilanguage experience applications create systems read document language render store another language also part document analysis system amazon translate fully integrate machine learn service amazon comprehend amazon transcribe amazon poly integration extract name entities sentiment phrase integrate amazon comprehend create multilingual subtitle amazon transcribe speak translate content amazon poly common case amazon translate first case build international websites amazon translate quickly globalize websites amazon translate also use develop multilingual chatbots chatbots use create humanlike interface applications amazon translate create chatbot speak multiple languages another case software localization localization major cost software aim global audience amazon translate decrease software development time significantly reduce cost localize software final example case international media management company manage media global audience use amazon translate reduce cost localization amazon comprehend implement many techniques review earlier module extract entities perform sentiment analysis word part speech common case amazon comprehend first example analyze legal medical document legal insurance medical organizations use amazon comprehend perform many function review module another largescale mobile analysis mobile developers amazon comprehend look pattern usage apps design improvements financial fraud detection another case amazon comprehend bank financial institutions use examine large data set financial transactions uncover fraud look pattern illegal transactions finally use content management media content company amazon comprehend content analysis management amazon human language front applications amazon let conversational engine power amazon alexa automatically increase capacity amazon solution create lambda function scale demand also store file conversations analysis common case amazon first case build front interfaces inventory management sales voice interfaces become common company use amazon chatbots inventory sales applications another amazon create customer service interfaces human like voice applications quickly become standard many customer service applications amazon reduce time take develop chatbots increase quality amazon like also use develop interactive assistance combine amazon service customers create sophisticate assistance many different industries final example case query databases humanlike language amazon combine database service create sophisticate data analysis applications human like language interface main point take away module first amazon transcribe automatically convert speak language amazon poly convert write text speak language amazon translate create realtime translation languages amazon comprehend automate many case review module finally amazon create humanlike interface applications thank watch well next video welcome back section well review five management machine learn service various case service simplify process create machine learn application start look amazon transcribe amazon transcribe recognize speech audio file produce transcription recognize specific voice audio file create customize vocabulary term specialize particular domain also transcription service applications integrate websockets internet protocol twoway communication application amazon transcribe common case amazon transcribe first medical professionals record note amazon transcribe capture speak note text also video production organizations generate subtitle automatically video could also do real time live fee close caption media company amazon transcribe capture label content fee content amazon comprehend analysis last company record customer service sales call transcribe analyze result train strategic opportunities amazon poly convert text lifelike speech input either plain text file file thats format speech synthesis markup language ssml ssml markup language use provide special instructions speech sound example introduce pause flow speech ssml instruct amazon poly pause word also output speech amazon poly vorbis audio stream format amazon poly eligible certain regulate workloads example eligible health insurance portability accountability 1996 hipaa amazon poly also eligible payment card industry data security standard common case amazon poly first example major news company use amazon poly generate vocal content directly write stories also embed map apis developers voice geob vacation language train company use amazon poly create systems learn language finally animators use voice character amazon translate create multilanguage experience applications create systems read document language render store another language also part document analysis system amazon translate fully integrate machine learn service amazon comprehend amazon transcribe amazon poly integration extract name entities sentiment phrase integrate amazon comprehend create multilingual subtitle amazon transcribe speak translate content amazon poly common case amazon translate first case build international websites amazon translate quickly globalize websites amazon translate also use develop multilingual chatbots chatbots use create humanlike interface applications amazon translate create chatbot speak multiple languages another case software localization localization major cost software aim global audience amazon translate decrease software development time significantly reduce cost localize software final example case international media management company manage media global audience use amazon translate reduce cost localization amazon comprehend implement many techniques review earlier module extract entities perform sentiment analysis word part speech common case amazon comprehend first example analyze legal medical document legal insurance medical organizations use amazon comprehend perform many function review module another largescale mobile analysis mobile developers amazon comprehend look pattern usage apps design improvements financial fraud detection another case amazon comprehend bank financial institutions use examine large data set financial transactions uncover fraud look pattern illegal transactions finally use content management media content company amazon comprehend content analysis management amazon human language front applications amazon let conversational engine power amazon alexa automatically increase capacity amazon solution create lambda function scale demand also store file conversations analysis common case amazon first case build front interfaces inventory management sales voice interfaces become common company use amazon chatbots inventory sales applications another amazon create customer service interfaces human like voice applications quickly become standard many customer service applications amazon reduce time take develop chatbots increase quality amazon like also use develop interactive assistance combine amazon service customers create sophisticate assistance many different industries final example case query databases humanlike language amazon combine database service create sophisticate data analysis applications human like language interface main point take away module first amazon transcribe automatically convert speak language amazon poly convert write text speak language amazon translate create realtime translation languages amazon comprehend automate many case review module finally amazon create humanlike interface applications thank watch well next video welcome back section go look host use model section well look deploy train model consume applications youve train tune test model youll learn test next section youre ready deploy model youre think look phase order heres discuss deployment want test model performance metrics first need make inference prediction model typically require deployment deployment test different production although mechanics amazon sage maker provide everything need host model simple test evaluation request deployments handle tens thousands request ways deploy model single predictions deploy model amazon sagemaker host service sagemaker deploy multiple compute instance model behind load balance endpoint applications call endpoint make predictions model scale number instance base demand predictions entire data amazon sagemaker batch transform instead deploy maintain permanent endpoint sagemaker spin model perform predictions entire data provide store result amazon shut terminate compute instance useful perform batch predictions test model quickly entire validation model without write code process collate individual result goal deployment phase provide manage environment host model provide inference securely latency model deploy production monitor production data retrain model necessary newly deploy model need reflect current production data data accumulate time could potentially identify alternative outcomes deploy model onetime exercise instead continuous process click deploy model amazon instance automatically scale across multiple availability zone higher redundancy specify type instance maximum minimum number instance desire sagemaker take care rest launch instance deploy model cure https endpoint application application need include call endpoint achieve inference latency high throughput architecture integrate model application minutes change model longer need change application code sagemaker manage production compute infrastructure behalf perform health check apply security patch conduct routine maintenance builtin amazon cloudwatch monitor log youve train model create endpoint either code use sagemaker console youre plan host single model create endpoint model youre plan host multiple model need create multi model endpoint multimodel endpoints scalable costeffective solution deploy large number model share serve container thats enable host multiple model reduce host cost improve endpoint utilization compare use single model endpoints also reduce deployment overhead sagemaker manage load model memory scale model base traffic pattern deploy machine learn model production make predictions data need make sure apply data process step use train inference request otherwise incorrect prediction result use inference pipelines reuse data process step model train inference without maintain separate copy code help ensure accuracy predictions reduce development sagemaker manage service inference pipelines completely manage deploy pipeline model service install run sequence containers instance endpoint batch transform additionally sequence feature process inference run latency containers collate instance takeaways section module include point deploy train model use sagemaker handle call applications perform predictions use batch transformation goal model generate predictions answer business problem sure model generate good result deploy production finally multimodal endpoint support save resources multiple model thats section well next video welcome back section go look host use model section well look deploy train model consume applications youve train tune test model youll learn test next section youre ready deploy model youre think look phase order heres discuss deployment want test model performance metrics first need make inference prediction model typically require deployment deployment test different production although mechanics amazon sage maker provide everything need host model simple test evaluation request deployments handle tens thousands request ways deploy model single predictions deploy model amazon sagemaker host service sagemaker deploy multiple compute instance model behind load balance endpoint applications call endpoint make predictions model scale number instance base demand predictions entire data amazon sagemaker batch transform instead deploy maintain permanent endpoint sagemaker spin model perform predictions entire data provide store result amazon shut terminate compute instance useful perform batch predictions test model quickly entire validation model without write code process collate individual result goal deployment phase provide manage environment host model provide inference securely latency model deploy production monitor production data retrain model necessary newly deploy model need reflect current production data data accumulate time could potentially identify alternative outcomes deploy model onetime exercise instead continuous process click deploy model amazon instance automatically scale across multiple availability zone higher redundancy specify type instance maximum minimum number instance desire sagemaker take care rest launch instance deploy model cure https endpoint application application need include call endpoint achieve inference latency high throughput architecture integrate model application minutes change model longer need change application code sagemaker manage production compute infrastructure behalf perform health check apply security patch conduct routine maintenance builtin amazon cloudwatch monitor log youve train model create endpoint either code use sagemaker console youre plan host single model create endpoint model youre plan host multiple model need create multi model endpoint multimodel endpoints scalable costeffective solution deploy large number model share serve container thats enable host multiple model reduce host cost improve endpoint utilization compare use single model endpoints also reduce deployment overhead sagemaker manage load model memory scale model base traffic pattern deploy machine learn model production make predictions data need make sure apply data process step use train inference request otherwise incorrect prediction result use inference pipelines reuse data process step model train inference without maintain separate copy code help ensure accuracy predictions reduce development sagemaker manage service inference pipelines completely manage deploy pipeline model service install run sequence containers instance endpoint batch transform additionally sequence feature process inference run latency containers collate instance takeaways section module include point deploy train model use sagemaker handle call applications perform predictions use batch transformation goal model generate predictions answer business problem sure model generate good result deploy production finally multimodal endpoint support save resources multiple model thats section well next video welcome back section go give quick highlevel overview machine learn terminology typical workflow cover topics detail later course well focus larger picture begin always start business problem team believe could benefit machine learn want problem formulation phase task articulate business problem convert problem formulate problem move data preparation preprocessing phase youll pull data data source data source might differences data type need reconcile form single cohesive view data go visualize data statistics determine data consistent use machine learn well look data source later course example data four columns contain data three different data source source slightly different ways represent data result show table problems columns represent feature row represent instance issue data instance case youll need subject matter expert functional expert understand authenticity data example date thats represent 1969 could november february 11th year 1969 someone own manage data pool would able clarify ambiguity also word mail probably attribute import issue cells shift position could outside chance actual location molly city thats capital republic maldives time error identification isnt simple youll need review data youll learn role experts later course remember largest impact success machine learn project consistent correct data data good shape time train model process get iterative fluid youll likely many multiple pass feature engineer train evaluate tune find model meet business goals feature engineer process select create feature model train feature columns data goal model correctly estimate target value data algorithm feature predict target example target data average number step take week select correct feature involve add remove calculate feature might want make data format consistent consistent format could later use model make change cosmetic reason depend problem want solve data might even need include name feature example data country feature traditional database might want move country lookup table reference algorithms want data instance single algorithms need numerical data process could consider turn country countrys code however model might interpret numerical value mean code value would significant isocode value case split data multiple columns fine know categorical encode youll learn later course type data could convert text value numerical value example could represent male female numeric value use easily model remain feature like birth month show table week show extract birth month week might appropriate depend problem youre try solve impact target variable week bear dont worry sound complicate youll learn feature engineer later course data clean youve identify feature want time train model wont data train model fact need hold data data test typically youll data train youll save rest data test next youll train model train data diagram model use xgboost algorithm model parameters parameters alter algorithm work theyre know hyperparameters output train train model train model test data well model perform youll take instance model hasnt see perform prediction already know target test data compare value comparisons calculate metrics give data well model perform youll make change model data feature hyperparameters find model yield best result train model theres real danger overfitting underfitting model model overfitting train data model perform well train data doesnt perform well evaluation data model memorize data cant generalize unseen examples model underfitting train data model perform poorly train data model cant capture relationship input examples often call target value often call understand model important understand root cause poor model accuracy understand guide take corrective step determine whether predictive model underfitting overfitting train data look prediction error train data evaluation data well show step take avoid later course youve retrain model youre satisfy result deploy model deliver best possible predictions later course well walk different phase give handson experience know process also useful use manage service also explore later certain amazon service bulk work takeaways section first look machine learn pipeline process process train evaluate model iterative process break three broad step data process model train model evaluation thats video well next welcome back section go give quick highlevel overview machine learn terminology typical workflow cover topics detail later course well focus larger picture begin always start business problem team believe could benefit machine learn want problem formulation phase task articulate business problem convert problem formulate problem move data preparation preprocessing phase youll pull data data source data source might differences data type need reconcile form single cohesive view data go visualize data statistics determine data consistent use machine learn well look data source later course example data four columns contain data three different data source source slightly different ways represent data result show table problems columns represent feature row represent instance issue data instance case youll need subject matter expert functional expert understand authenticity data example date thats represent 1969 could november february 11th year 1969 someone own manage data pool would able clarify ambiguity also word mail probably attribute import issue cells shift position could outside chance actual location molly city thats capital republic maldives time error identification isnt simple youll need review data youll learn role experts later course remember largest impact success machine learn project consistent correct data data good shape time train model process get iterative fluid youll likely many multiple pass feature engineer train evaluate tune find model meet business goals feature engineer process select create feature model train feature columns data goal model correctly estimate target value data algorithm feature predict target example target data average number step take week select correct feature involve add remove calculate feature might want make data format consistent consistent format could later use model make change cosmetic reason depend problem want solve data might even need include name feature example data country feature traditional database might want move country lookup table reference algorithms want data instance single algorithms need numerical data process could consider turn country countrys code however model might interpret numerical value mean code value would significant isocode value case split data multiple columns fine know categorical encode youll learn later course type data could convert text value numerical value example could represent male female numeric value use easily model remain feature like birth month show table week show extract birth month week might appropriate depend problem youre try solve impact target variable week bear dont worry sound complicate youll learn feature engineer later course data clean youve identify feature want time train model wont data train model fact need hold data data test typically youll data train youll save rest data test next youll train model train data diagram model use xgboost algorithm model parameters parameters alter algorithm work theyre know hyperparameters output train train model train model test data well model perform youll take instance model hasnt see perform prediction already know target test data compare value comparisons calculate metrics give data well model perform youll make change model data feature hyperparameters find model yield best result train model theres real danger overfitting underfitting model model overfitting train data model perform well train data doesnt perform well evaluation data model memorize data cant generalize unseen examples model underfitting train data model perform poorly train data model cant capture relationship input examples often call target value often call understand model important understand root cause poor model accuracy understand guide take corrective step determine whether predictive model underfitting overfitting train data look prediction error train data evaluation data well show step take avoid later course youve retrain model youre satisfy result deploy model deliver best possible predictions later course well walk different phase give handson experience know process also useful use manage service also explore later certain amazon service bulk work takeaways section first look machine learn pipeline process process train evaluate model iterative process break three broad step data process model train model evaluation thats video well next welcome module academy machine learn module go introduce machine learn well first look business problems solve machine learn talk terminology process tool challenge youll face complete module able recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods youre ready start section welcome module academy machine learn module go introduce machine learn well first look business problems solve machine learn talk terminology process tool challenge youll face complete module able recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods youre ready start section welcome module academy machine learn module go introduce machine learn well first look business problems solve machine learn talk terminology process tool challenge youll face complete module able recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods youre ready start section welcome module academy machine learn module go introduce machine learn well first look business problems solve machine learn talk terminology process tool challenge youll face complete module able recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods youre ready start section welcome module academy machine learn module go introduce machine learn well first look business problems solve machine learn talk terminology process tool challenge youll face complete module able recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods youre ready start section welcome module academy machine learn module go introduce machine learn well first look business problems solve machine learn talk terminology process tool challenge youll face complete module able recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods youre ready start section welcome module academy machine learn module go introduce machine learn well first look business problems solve machine learn talk terminology process tool challenge youll face complete module able recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve business problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods youre ready start section time summarize main point module module learn describe case computer vision describe amazon management machine learn service available image video analysis list step require prepare custom data object detection describe amazon sagemaker grind truth use prepare custom data amazon recognition perform facial detection conclude introduction computer vision thank watch well next video time summarize main point module module learn describe case computer vision describe amazon management machine learn service available image video analysis list step require prepare custom data object detection describe amazon sagemaker grind truth use prepare custom data amazon recognition perform facial detection conclude introduction computer vision thank watch well next video welcome section section go look feature engineer feature engineer impactful things improve machine learn model well look things help make model successful first feature selection second feature extraction process create feature feature selection select relevant feature discard rest apply feature selection prevent redundancy irrelevance exist feature also limit number feature help prevent overfitting feature extraction build valuable information data reformatting combine transform primary feature ones process continue yield data consume model achieve goals diagram show feature extraction cover range activities deal miss data convert text data numerical data although list isnt exhaustive give idea data handle thats need data useful state many task different work data youll want make sure data correct format consistently represent correctly spell among task example might combine data extract data multiple columns could also remove columns together specific machine learn youll need convert text columns numerical value also need decide handle outliers potentially rescale data next well look common task section machine learn algorithms work best numerical data youll need make sure columns data contain numeric data convert encode might need make several pass data sheet encode example might variability text value row contain medium value categorical data order youll want encode text numerical value capture ordinal relationship data show maintenance cost might encode medium high three four youve make sure categorical data uniform tool like skykit learn pandas encode data categorical data doesnt order youll need break data multiple columns help make sure dont introduce ordinal relationship data isnt example suppose assign value first color assign next value blue model could interpret blue important blue higher numeric value encode nonordinal data multiple columns feature better think feature like checkbox consider example three feature generate value indicate instance feature like color thats section well next video welcome section section go look feature engineer feature engineer impactful things improve machine learn model well look things help make model successful first feature selection second feature extraction process create feature feature selection select relevant feature discard rest apply feature selection prevent redundancy irrelevance exist feature also limit number feature help prevent overfitting feature extraction build valuable information data reformatting combine transform primary feature ones process continue yield data consume model achieve goals diagram show feature extraction cover range activities deal miss data convert text data numerical data although list isnt exhaustive give idea data handle thats need data useful state many task different work data youll want make sure data correct format consistently represent correctly spell among task example might combine data extract data multiple columns could also remove columns together specific machine learn youll need convert text columns numerical value also need decide handle outliers potentially rescale data next well look common task section machine learn algorithms work best numerical data youll need make sure columns data contain numeric data convert encode might need make several pass data sheet encode example might variability text value row contain medium value categorical data order youll want encode text numerical value capture ordinal relationship data show maintenance cost might encode medium high three four youve make sure categorical data uniform tool like skykit learn pandas encode data categorical data doesnt order youll need break data multiple columns help make sure dont introduce ordinal relationship data isnt example suppose assign value first color assign next value blue model could interpret blue important blue higher numeric value encode nonordinal data multiple columns feature better think feature like checkbox consider example three feature generate value indicate instance feature like color thats section well next video time review module wrap knowledge check module learn formulate problem business request obtain secure data machine learn build jupiter notebook use amazon sage maker outline process evaluate data explain data need preprocessed open source tool examine preprocess data amazon sagemaker train host machine learn model cross validation test performance model host model inference create amazon sagemaker hyperparameter tune optimize model effectiveness conclude module thank watch well next video time review module wrap knowledge check module learn formulate problem business request obtain secure data machine learn build jupiter notebook use amazon sage maker outline process evaluate data explain data need preprocessed open source tool examine preprocess data amazon sagemaker train host machine learn model cross validation test performance model host model inference create amazon sagemaker hyperparameter tune optimize model effectiveness conclude module thank watch well next video welcome back section go look type business problems machine learn help solve machine learn use across digital live email spam filter result machine learn program train examples spam regular email message base book youre read products buy machine learn program predict book products likely interest machine learn program train data readers habit purchase detect credit card fraud machine learn program train examples transactions turn fraud along normal transactions probably think many examples social media applications use facial detection group photos detect brain tumors brain scan find normally xrays three main type machine learn theyre supervise learn model use know input output generalize future output theres unsupervised learn model doesnt know input output find pattern data without help theres reinforcement learn model interact environment learn take action maximize reward important know different type type guide towards select algorithms make sense solve business problem let look type supervise learn popular type widely applicable call supervise learn need supervisor teacher show right answer speak like student supervise algorithm need learn example essentially need teacher use train data help determine pattern relationships input output want build application detect credit card fraud need train data include examples fraud examples normal transactions within supervise learn different type problems classification regression subtypes classification problems first binary classification think back example identify fraudulent transactions target variable example limit options fraudulent fraudulent binary classification problem also multiclass classification problems problems classify observation three categories model customer call store reduce number transfer need customer get correct customer support department case different customer support departments represent variety potential target variables could many different departments much also regression problems regression problem youre longer map input define number categories instead youre map input continuous value like integer example regression problem predict price company stock computer vision good example supervise learn tumor xray computer vision often build deep learn model automate extraction analysis classification understand useful information single image sequence image computer vision enable machine identify people place things image accuracy human level greater speed efficiency image data take many form single image video sequence view multiple cameras threedimensional data youll learn computer vision later course well discuss unsupervised machine learn sometimes data theres supervisor room unsupervised learn label arent provide like supervise learn dont know variables pattern instance machine uncover create label model data theyre present detect emerge properties entire data construct pattern properties cluster common subcategory unsupervised learn kind algorithm group data different cluster base similar feature better understand attribute specific cluster example analyze customer purchase habit unsupervised algorithms identify group customers associate size tier company advantage unsupervised algorithms enable pattern data werent aware natural language process also know another area machine learn thats experience growth youve ever use alexa voice assistant theyll answer question isnt speech also write text show many applications example use chat call center bots automate systems help bank balance order food restaurant translation tool convert text languages example might applications translate menus real time also use voice text translations convert speak word text finally use sentiment analysis analyze sentiment comment review products music movies sentiments could use give movie audience rat learn later course another kind machine learn thats gain popularity recently reinforcement learn unlike machine learn reinforcement learn continuously improve model mine feedback previous iterations reinforcement learn agent continuously learn trial error interact environment reinforcement learn broadly useful reward desire outcome know path achieve isnt path require trial error discover take example deep racer deep racer simulator agent virtual environment virtual race track action throttle steer input goal complete racetrack quickly possible without deviate track need learn desire drive behavior reach goal complete track learn deepracer team reward incentivize model learn desire drive behavior reinforcement learn drive learn call agent case deep racer environment place agent learn example would mark race track agent something environment provoke response cross boundary shouldnt cross thats call action response call reward penalty depend whether agent something reinforce discourage model agent move within environment action start receive reward fewer penalties meet desire business outcome selfdriving vehicles bring together many machine deep learn algorithms model solve problem drive point point main task continuous detection environment forecast change detect object localize predict movement detect object output find input systems make decisions vehicles various control case selfdriving vehicles require realtime responses environment example previously hide pedestrian walk behind obstacle vehicle break need apply immediately latency room error action every problem solve machine learn sometimes regular program work well need youre interest explore potential machine learn solution look existence large data set large number variables machine learn often best choice youre uncertain business logic procedures need obtain answer accomplish task machine learn systems plex support infrastructure management support technical expertise need place help ensure project success takeaways section explore machine learn applications already part everyday life first machine learn problems group three categories supervise learn train data already know answer unsupervised learn data look insights within data reinforcement learn model learn base experience feedback business problems supervise learn problems thats section well next video welcome back section go look type business problems machine learn help solve machine learn use across digital live email spam filter result machine learn program train examples spam regular email message base book youre read products buy machine learn program predict book products likely interest machine learn program train data readers habit purchase detect credit card fraud machine learn program train examples transactions turn fraud along normal transactions probably think many examples social media applications use facial detection group photos detect brain tumors brain scan find normally xrays three main type machine learn theyre supervise learn model use know input output generalize future output theres unsupervised learn model doesnt know input output find pattern data without help theres reinforcement learn model interact environment learn take action maximize reward important know different type type guide towards select algorithms make sense solve business problem let look type supervise learn popular type widely applicable call supervise learn need supervisor teacher show right answer speak like student supervise algorithm need learn example essentially need teacher use train data help determine pattern relationships input output want build application detect credit card fraud need train data include examples fraud examples normal transactions within supervise learn different type problems classification regression subtypes classification problems first binary classification think back example identify fraudulent transactions target variable example limit options fraudulent fraudulent binary classification problem also multiclass classification problems problems classify observation three categories model customer call store reduce number transfer need customer get correct customer support department case different customer support departments represent variety potential target variables could many different departments much also regression problems regression problem youre longer map input define number categories instead youre map input continuous value like integer example regression problem predict price company stock computer vision good example supervise learn tumor xray computer vision often build deep learn model automate extraction analysis classification understand useful information single image sequence image computer vision enable machine identify people place things image accuracy human level greater speed efficiency image data take many form single image video sequence view multiple cameras threedimensional data youll learn computer vision later course well discuss unsupervised machine learn sometimes data theres supervisor room unsupervised learn label arent provide like supervise learn dont know variables pattern instance machine uncover create label model data theyre present detect emerge properties entire data construct pattern properties cluster common subcategory unsupervised learn kind algorithm group data different cluster base similar feature better understand attribute specific cluster example analyze customer purchase habit unsupervised algorithms identify group customers associate size tier company advantage unsupervised algorithms enable pattern data werent aware natural language process also know another area machine learn thats experience growth youve ever use alexa voice assistant theyll answer question isnt speech also write text show many applications example use chat call center bots automate systems help bank balance order food restaurant translation tool convert text languages example might applications translate menus real time also use voice text translations convert speak word text finally use sentiment analysis analyze sentiment comment review products music movies sentiments could use give movie audience rat learn later course another kind machine learn thats gain popularity recently reinforcement learn unlike machine learn reinforcement learn continuously improve model mine feedback previous iterations reinforcement learn agent continuously learn trial error interact environment reinforcement learn broadly useful reward desire outcome know path achieve isnt path require trial error discover take example deep racer deep racer simulator agent virtual environment virtual race track action throttle steer input goal complete racetrack quickly possible without deviate track need learn desire drive behavior reach goal complete track learn deepracer team reward incentivize model learn desire drive behavior reinforcement learn drive learn call agent case deep racer environment place agent learn example would mark race track agent something environment provoke response cross boundary shouldnt cross thats call action response call reward penalty depend whether agent something reinforce discourage model agent move within environment action start receive reward fewer penalties meet desire business outcome selfdriving vehicles bring together many machine deep learn algorithms model solve problem drive point point main task continuous detection environment forecast change detect object localize predict movement detect object output find input systems make decisions vehicles various control case selfdriving vehicles require realtime responses environment example previously hide pedestrian walk behind obstacle vehicle break need apply immediately latency room error action every problem solve machine learn sometimes regular program work well need youre interest explore potential machine learn solution look existence large data set large number variables machine learn often best choice youre uncertain business logic procedures need obtain answer accomplish task machine learn systems plex support infrastructure management support technical expertise need place help ensure project success takeaways section explore machine learn applications already part everyday life first machine learn problems group three categories supervise learn train data already know answer unsupervised learn data look insights within data reinforcement learn model learn base experience feedback business problems supervise learn problems thats section well next video welcome module course wrap congratulations complete academy machine learn course well take minutes review youve learn go start review youve learn course learn describe machine learn implement machine learn pipeline amazon machine learn service forecast computer vision natural language process well do although course isnt design prepare become certify certify machine learn specialty review continue work towards certification certification help build credibility confidence validate cloud expertise industry recognize credential also help organizations identify skilled professional lead cloud initiatives use must earn pass score take proctor exam earn certification receive pass score youll receive certification credentials certification doesnt publish list service feature cover certification exam however theres exam guide exam list current topic areas objectives cover exam exam guide find prepare certification exam page youll require update certification recertify every years view certification recertification page detail information slide current june 2020 however exams frequently update also detail start exams available topics test exam subject change certify machine learn specialty mean select justify appropriate machine learn approach give business problem also identify appropriate service implement machine learn solutions finally design implement scalable cost optimize reliable secure machine learn solutions sit certify machine learn specialty exam recommend follow knowledge experience first years experience develop architecting run deep learn workloads cloud experience include perform basic hyperparameter optimization work machine learn learn frameworks also able express intuition behind basic algorithms finally able follow best practice model train addition best practice deployment operations thank watch congratulations complete academy machine learn course welcome module course wrap congratulations complete academy machine learn course well take minutes review youve learn go start review youve learn course learn describe machine learn implement machine learn pipeline amazon machine learn service forecast computer vision natural language process well do although course isnt design prepare become certify certify machine learn specialty review continue work towards certification certification help build credibility confidence validate cloud expertise industry recognize credential also help organizations identify skilled professional lead cloud initiatives use must earn pass score take proctor exam earn certification receive pass score youll receive certification credentials certification doesnt publish list service feature cover certification exam however theres exam guide exam list current topic areas objectives cover exam exam guide find prepare certification exam page youll require update certification recertify every years view certification recertification page detail information slide current june 2020 however exams frequently update also detail start exams available topics test exam subject change certify machine learn specialty mean select justify appropriate machine learn approach give business problem also identify appropriate service implement machine learn solutions finally design implement scalable cost optimize reliable secure machine learn solutions sit certify machine learn specialty exam recommend follow knowledge experience first years experience develop architecting run deep learn workloads cloud experience include perform basic hyperparameter optimization work machine learn learn frameworks also able express intuition behind basic algorithms finally able follow best practice model train addition best practice deployment operations thank watch congratulations complete academy machine learn course time review module main takeaways module first look define machine learn fit broader landscape also look type problem machine learn help solve machine learn apply learn algorithms develop model large data set look machine learn pipeline different stag develop machine learn application finally introduce tool service discuss challenge machine learn summary module learn recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods thank watch well next video time review module main takeaways module first look define machine learn fit broader landscape also look type problem machine learn help solve machine learn apply learn algorithms develop model large data set look machine learn pipeline different stag develop machine learn application finally introduce tool service discuss challenge machine learn summary module learn recognize machine learn deep learn part artificial intelligence describe artificial intelligence machine learn terminology identify machine learn use solve problem describe machine learn process list tool available data scientists identify machine learn instead traditional software development methods thank watch well next video welcome module academy machine learn introduction natural language process module well introduce natural language process also know section include description major challenge face overall development process applications review five service speed development nlpbased applications complete module able describe case solve use manage amazon service describe manage amazon service available let start welcome module academy machine learn introduction natural language process module well introduce natural language process also know section include description major challenge face overall development process applications review five service speed development nlpbased applications complete module able describe case solve use manage amazon service describe manage amazon service available let start welcome section well start review forecast case forecast important area machine learn important many opportunities predict future outcomes base historical data many opportunities involve time component however time component add additional information also make time serious problems difficult handle compare type predictions think time series data fall broad categories first type univariate data mean theres variable second multivariate data mean theres variable several common pattern time series data first pattern trend trend pattern value increase decrease stay time seasonal pattern reflect time year month pattern cyclical pattern similar seasonal pattern pattern repeat like large retail sale event happen time year finally change data time appear random discernible pattern many use forecast forecast market applications sales forecast demand projections could also use inventory management systems anticipate require inventory level forecast energy consumption help predict need weather forecast systems use governments commercial applications agriculture thats section next video welcome section well start review forecast case forecast important area machine learn important many opportunities predict future outcomes base historical data many opportunities involve time component however time component add additional information also make time serious problems difficult handle compare type predictions think time series data fall broad categories first type univariate data mean theres variable second multivariate data mean theres variable several common pattern time series data first pattern trend trend pattern value increase decrease stay time seasonal pattern reflect time year month pattern cyclical pattern similar seasonal pattern pattern repeat like large retail sale event happen time year finally change data time appear random discernible pattern many use forecast forecast market applications sales forecast demand projections could also use inventory management systems anticipate require inventory level forecast energy consumption help predict need weather forecast systems use governments commercial applications agriculture thats section next video welcome section well start review forecast case forecast important area machine learn important many opportunities predict future outcomes base historical data many opportunities involve time component however time component add additional information also make time serious problems difficult handle compare type predictions think time series data fall broad categories first type univariate data mean theres variable second multivariate data mean theres variable several common pattern time series data first pattern trend trend pattern value increase decrease stay time seasonal pattern reflect time year month pattern cyclical pattern similar seasonal pattern pattern repeat like large retail sale event happen time year finally change data time appear random discernible pattern many use forecast forecast market applications sales forecast demand projections could also use inventory management systems anticipate require inventory level forecast energy consumption help predict need weather forecast systems use governments commercial applications agriculture thats section next video welcome section well start review forecast case forecast important area machine learn important many opportunities predict future outcomes base historical data many opportunities involve time component however time component add additional information also make time serious problems difficult handle compare type predictions think time series data fall broad categories first type univariate data mean theres variable second multivariate data mean theres variable several common pattern time series data first pattern trend trend pattern value increase decrease stay time seasonal pattern reflect time year month pattern cyclical pattern similar seasonal pattern pattern repeat like large retail sale event happen time year finally change data time appear random discernible pattern many use forecast forecast market applications sales forecast demand projections could also use inventory management systems anticipate require inventory level forecast energy consumption help predict need weather forecast systems use governments commercial applications agriculture thats section next video welcome back section go cover evaluate data section well look different data format type well also look visualize analyze data feature engineer start run statistics data better understand youre work need ensure right format analysis amazon sage maker algorithm support train data format many tool youll explore visualize analyze data also read format generally speak youll need least domain knowledge problem youre try solve machine learn example youre develop model predict symptoms indicate disease need know relationship symptoms disease data typically need numeric form machine learn algorithms data make predictions well look ways convert text data next section well explore data gain insights overall data popular open source python library pandas take data various format reformat load tabular representation data present row columns format pandas reformat load include excel pickle javascript object notation json pandas also data analysis manipulation feature well throughout module load data simple example pull file specify hello data pandas store pandas data frame pandas documentation data frame describe general label size mutable tabular structure potentially heterogeneously type column helpful think data frame think spreadsheet table like table spreadsheet data frame row also know instance well columns also know attribute shape property data frame describe number row columns column data frame series series onedimensional label array series store data type learn data structure pandas pandas documentation along data load data frame column label label know index column label know columns load data file header columns create first line file change behavior however dont column name source file pass parameter perform data analysis important make sure youre use correct data type many case pandas correctly infer correct data type load data move domain knowledge access domain expert often identify data type issue either type info function obtain information column type show example dont correct data type need figure case often numeric column miss data could single text value example data number doors three four five analyze data convert column correct data type use pandas thats part section well part well review describe data welcome back section go cover evaluate data section well look different data format type well also look visualize analyze data feature engineer start run statistics data better understand youre work need ensure right format analysis amazon sage maker algorithm support train data format many tool youll explore visualize analyze data also read format generally speak youll need least domain knowledge problem youre try solve machine learn example youre develop model predict symptoms indicate disease need know relationship symptoms disease data typically need numeric form machine learn algorithms data make predictions well look ways convert text data next section well explore data gain insights overall data popular open source python library pandas take data various format reformat load tabular representation data present row columns format pandas reformat load include excel pickle javascript object notation json pandas also data analysis manipulation feature well throughout module load data simple example pull file specify hello data pandas store pandas data frame pandas documentation data frame describe general label size mutable tabular structure potentially heterogeneously type column helpful think data frame think spreadsheet table like table spreadsheet data frame row also know instance well columns also know attribute shape property data frame describe number row columns column data frame series series onedimensional label array series store data type learn data structure pandas pandas documentation along data load data frame column label label know index column label know columns load data file header columns create first line file change behavior however dont column name source file pass parameter perform data analysis important make sure youre use correct data type many case pandas correctly infer correct data type load data move domain knowledge access domain expert often identify data type issue either type info function obtain information column type show example dont correct data type need figure case often numeric column miss data could single text value example data number doors three four five analyze data convert column correct data type use pandas thats part section well part well review describe data welcome back section go cover evaluate data section well look different data format type well also look visualize analyze data feature engineer start run statistics data better understand youre work need ensure right format analysis amazon sage maker algorithm support train data format many tool youll explore visualize analyze data also read format generally speak youll need least domain knowledge problem youre try solve machine learn example youre develop model predict symptoms indicate disease need know relationship symptoms disease data typically need numeric form machine learn algorithms data make predictions well look ways convert text data next section well explore data gain insights overall data popular open source python library pandas take data various format reformat load tabular representation data present row columns format pandas reformat load include excel pickle javascript object notation json pandas also data analysis manipulation feature well throughout module load data simple example pull file specify hello data pandas store pandas data frame pandas documentation data frame describe general label size mutable tabular structure potentially heterogeneously type column helpful think data frame think spreadsheet table like table spreadsheet data frame row also know instance well columns also know attribute shape property data frame describe number row columns column data frame series series onedimensional label array series store data type learn data structure pandas pandas documentation along data load data frame column label label know index column label know columns load data file header columns create first line file change behavior however dont column name source file pass parameter perform data analysis important make sure youre use correct data type many case pandas correctly infer correct data type load data move domain knowledge access domain expert often identify data type issue either type info function obtain information column type show example dont correct data type need figure case often numeric column miss data could single text value example data number doors three four five analyze data convert column correct data type use pandas thats part section well part well review describe data welcome back section go cover evaluate data section well look different data format type well also look visualize analyze data feature engineer start run statistics data better understand youre work need ensure right format analysis amazon sage maker algorithm support train data format many tool youll explore visualize analyze data also read format generally speak youll need least domain knowledge problem youre try solve machine learn example youre develop model predict symptoms indicate disease need know relationship symptoms disease data typically need numeric form machine learn algorithms data make predictions well look ways convert text data next section well explore data gain insights overall data popular open source python library pandas take data various format reformat load tabular representation data present row columns format pandas reformat load include excel pickle javascript object notation json pandas also data analysis manipulation feature well throughout module load data simple example pull file specify hello data pandas store pandas data frame pandas documentation data frame describe general label size mutable tabular structure potentially heterogeneously type column helpful think data frame think spreadsheet table like table spreadsheet data frame row also know instance well columns also know attribute shape property data frame describe number row columns column data frame series series onedimensional label array series store data type learn data structure pandas pandas documentation along data load data frame column label label know index column label know columns load data file header columns create first line file change behavior however dont column name source file pass parameter perform data analysis important make sure youre use correct data type many case pandas correctly infer correct data type load data move domain knowledge access domain expert often identify data type issue either type info function obtain information column type show example dont correct data type need figure case often numeric column miss data could single text value example data number doors three four five analyze data convert column correct data type use pandas thats part section well part well review describe data\n"
     ]
    }
   ],
   "source": [
    "desitnation_folder = './transcriptions'\n",
    "\n",
    "norm_text = ''\n",
    "for file in os.listdir(desitnation_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file = os.path.join(desitnation_folder,file)\n",
    "        with open(file, \"r\") as file:\n",
    "            text = file.read()\n",
    "            text = text.split()\n",
    "            text = clean_text(text)\n",
    "            norm_text = \" \".join([norm_text,text])\n",
    "print(norm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting key phrases and topics\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to extract the key phrases and topics from the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 07:14:55.692469: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Write your answer/code here\n",
    "from keybert import KeyBERT\n",
    "\n",
    "def get_keyphrases(full_text):\n",
    "    kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
    "    keywords = kw_model.extract_keywords(full_text, \n",
    "\n",
    "                                         keyphrase_ngram_range=(1, 3), \n",
    "\n",
    "                                         stop_words='english', \n",
    "\n",
    "                                         highlight=False,\n",
    "\n",
    "                                         top_n=50)\n",
    "    keywords_list= list(dict(keywords).keys())\n",
    "    return keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine learn business', 'youre develop model', 'create systems learn', 'create machine learn', 'solution implement business', 'introduce machine learn', 'machine learn project', 'develop model', 'machine learn development', 'guidance formulate business', 'machine learn need', 'requirement machine', 'develop machine learn', 'module learn business', 'analysis data engineer', 'develop model predict', 'develop solution', 'machine learn engineer', 'solution meet businesss', 'business requirement machine', 'module youll introduction', 'machine learn build', 'engineer machine learn', 'need assess model', 'create sophisticate assistance', 'implement machine learn', 'learn machine learn', 'machine learn model', 'machine learn want', 'learn provide various', 'handle machine learn', 'use machine learn', 'problem data preparation', 'machine learn developer', 'business problem machine', 'data scientists machine', 'business problems data', 'problem predict', 'data opportunities involve', 'learn model use', 'data teach model', 'engineer feature engineer', 'course develop solution', 'machine learn professionals', 'data feature engineer', 'look machine learn', 'machine learn implement', 'learn algorithms develop', 'machine learn framework', 'algorithm support train']\n"
     ]
    }
   ],
   "source": [
    "keyphrase = get_keyphrases(norm_text)\n",
    "print(keyphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_biagram(data,tokens):\n",
    "    bigram = gensim.models.Phrases(data, min_count=20, threshold=100) # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in tokens]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(data,num_of_topics):\n",
    "\n",
    "    data = data.split(' ')\n",
    "    tokens = []\n",
    "    for text in data:\n",
    "        text = word_tokenize(text)\n",
    "        tokens.append(text)\n",
    "        \n",
    "    # Make Biagrams\n",
    "    tokens = make_biagram(data=data,tokens=tokens)\n",
    "\n",
    "    # Corpora Dictionary\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "\n",
    "    lda_model =  gensim.models.LdaModel(doc_term_matrix,  \n",
    "                                       num_topics = num_of_topics,     \n",
    "                                       id2word = dictionary,                                    \n",
    "                                       passes = 2,\n",
    "                                       chunksize=10,\n",
    "                                       update_every=1,\n",
    "                                       alpha='auto',\n",
    "                                       per_word_topics=True,\n",
    "                                       random_state=42\n",
    "                                       )\n",
    "\n",
    "\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return lda_model,doc_term_matrix,dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.148*\"learn\" + 0.102*\"look\" + 0.028*\"column\" + 0.026*\"frame\" + 0.024*\"pandas\" + 0.021*\"load\" + 0.017*\"file\" + 0.015*\"domain\" + 0.011*\"format\" + 0.010*\"spreadsheet\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.041*\"type\" + 0.036*\"pandas\" + 0.031*\"column\" + 0.030*\"use\" + 0.028*\"analysis\" + 0.026*\"part\" + 0.024*\"describe\" + 0.023*\"many\" + 0.022*\"frame\" + 0.020*\"label\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.081*\"well\" + 0.046*\"know\" + 0.045*\"also\" + 0.034*\"type\" + 0.033*\"youre\" + 0.024*\"pandas\" + 0.022*\"column\" + 0.020*\"machine\" + 0.020*\"make\" + 0.019*\"review\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.083*\"need\" + 0.070*\"section\" + 0.032*\"pandas\" + 0.031*\"column\" + 0.027*\"thats\" + 0.026*\"type\" + 0.024*\"frame\" + 0.021*\"columns\" + 0.020*\"load\" + 0.016*\"correct\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.402*\"data\" + 0.044*\"example\" + 0.025*\"type\" + 0.020*\"case\" + 0.017*\"pandas\" + 0.016*\"youll\" + 0.015*\"column\" + 0.013*\"identify\" + 0.012*\"frame\" + 0.011*\"load\"\n",
      "\n",
      "\n",
      "whole topic list: LdaModel<num_terms=1707, num_topics=5, decay=0.5, chunksize=10>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_topics = 5\n",
    "lda_model,corpus,id2word = topic_modeling(norm_text,num_topics)\n",
    "print(\"whole topic list:\",lda_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating the dashboard\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to create the dashboard for your solution.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el524001396966012831842814629242\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el524001396966012831842814629242_data = {\"mdsDat\": {\"x\": [-0.04007450114163566, -0.02204247535664701, -0.1680899743260027, 0.045888356644264036, 0.18431859418002108], \"y\": [-0.13919225848863193, -0.06223558519611441, 0.13110084095348237, -0.015345462644438916, 0.08567246537570282], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [28.47092819784367, 26.656343240872953, 23.84978377182315, 14.853892782616478, 6.16905200684374]}, \"tinfo\": {\"Term\": [\"data\", \"well\", \"learn\", \"need\", \"section\", \"look\", \"also\", \"example\", \"know\", \"youre\", \"use\", \"analysis\", \"many\", \"thats\", \"machine\", \"make\", \"part\", \"feature\", \"review\", \"column\", \"youll\", \"frame\", \"first\", \"case\", \"identify\", \"amazon\", \"pandas\", \"describe\", \"load\", \"create\", \"well\", \"also\", \"youre\", \"machine\", \"make\", \"like\", \"module\", \"time\", \"include\", \"solve\", \"welcome\", \"train\", \"know\", \"process\", \"review\", \"algorithm\", \"source\", \"youve\", \"single\", \"base\", \"move\", \"attribute\", \"large\", \"perform\", \"algorithms\", \"convert\", \"instance\", \"applications\", \"text\", \"dont\", \"often\", \"label\", \"type\", \"correct\", \"format\", \"pandas\", \"columns\", \"column\", \"frame\", \"load\", \"file\", \"analyze\", \"part\", \"use\", \"analysis\", \"many\", \"feature\", \"first\", \"next\", \"take\", \"create\", \"model\", \"work\", \"different\", \"go\", \"video\", \"service\", \"problem\", \"part\", \"available\", \"cover\", \"understand\", \"number\", \"describe\", \"manage\", \"require\", \"object\", \"apply\", \"pass\", \"dont\", \"label\", \"engineer\", \"common\", \"correct\", \"type\", \"pandas\", \"column\", \"columns\", \"load\", \"frame\", \"think\", \"format\", \"file\", \"series\", \"analyze\", \"row\", \"domain\", \"often\", \"data\", \"example\", \"youll\", \"identify\", \"back\", \"help\", \"problems\", \"case\", \"takeaways\", \"find\", \"start\", \"development\", \"artificial\", \"management\", \"instance\", \"pattern\", \"language\", \"application\", \"thank\", \"challenge\", \"speak\", \"instead\", \"intelligence\", \"handle\", \"watch\", \"isnt\", \"deep\", \"point\", \"could\", \"recognize\", \"perform\", \"applications\", \"text\", \"forecast\", \"predictions\", \"think\", \"series\", \"information\", \"type\", \"convert\", \"however\", \"sure\", \"format\", \"pandas\", \"frame\", \"load\", \"column\", \"store\", \"file\", \"describe\", \"analyze\", \"domain\", \"label\", \"columns\", \"row\", \"correct\", \"documentation\", \"need\", \"section\", \"thats\", \"amazon\", \"finally\", \"terminology\", \"tool\", \"traditional\", \"face\", \"able\", \"methods\", \"scientists\", \"academy\", \"important\", \"complete\", \"five\", \"give\", \"intelligence\", \"guide\", \"relationship\", \"list\", \"introduction\", \"theres\", \"description\", \"explore\", \"overall\", \"major\", \"seasonal\", \"nlpbased\", \"year\", \"four\", \"correctly\", \"columns\", \"series\", \"value\", \"evaluate\", \"miss\", \"describe\", \"issue\", \"expert\", \"access\", \"either\", \"column\", \"info\", \"frame\", \"domain\", \"file\", \"row\", \"numeric\", \"pandas\", \"load\", \"figure\", \"correct\", \"table\", \"spreadsheet\", \"structure\", \"documentation\", \"often\", \"type\", \"label\", \"format\", \"analyze\", \"tabular\", \"reformat\", \"knowledge\", \"learn\", \"look\", \"precision\", \"chart\", \"modules\", \"limitation\", \"occur\", \"inverse\", \"qualify\", \"medicine\", \"entity\", \"noun\", \"disambiguate\", \"titanic\", \"north\", \"atlantic\", \"lack\", \"distance\", \"tablet\", \"adapt\", \"material\", \"come\", \"inconsistent\", \"portion\", \"enough\", \"representative\", \"mostly\", \"staff\", \"effective\", \"adopt\", \"least\", \"generally\", \"try\", \"symptoms\", \"disease\", \"statistics\", \"doors\", \"graph\", \"normalize\", \"stem\", \"visualize\", \"ways\", \"figure\", \"typically\", \"access\", \"either\", \"info\", \"function\", \"infer\", \"expert\", \"issue\", \"spreadsheet\", \"representation\", \"library\", \"python\", \"domain\", \"javascript\", \"json\", \"notation\", \"pickle\", \"excel\", \"array\", \"onedimensional\", \"table\", \"parameter\", \"frame\", \"file\", \"reformat\", \"load\", \"column\", \"tabular\", \"structure\", \"documentation\", \"pandas\", \"line\", \"format\", \"name\", \"row\", \"miss\"], \"Freq\": [2458.0, 588.0, 235.0, 315.0, 266.0, 162.0, 326.0, 266.0, 347.0, 238.0, 203.0, 194.0, 157.0, 102.0, 146.0, 144.0, 235.0, 114.0, 153.0, 622.0, 97.0, 477.0, 95.0, 165.0, 81.0, 57.0, 687.0, 282.0, 411.0, 77.0, 587.5067971109548, 325.7412548499927, 238.08754399888517, 146.06734128761227, 143.35924448177798, 76.14869537852556, 73.55110207625412, 71.16496290813687, 64.01653991028667, 59.31081234983908, 57.17875514306205, 53.395304471994656, 334.7376152188791, 8.040340129930057, 139.3214638551574, 45.33700769259468, 117.2400591306623, 1.4547244988189163, 53.75987974535473, 5.126376278743434, 48.90936772787487, 39.66325000906103, 6.03748026446277, 40.798129279773356, 30.569750646665103, 70.08849137035484, 31.06209729461025, 9.977981763857622, 63.671202872928696, 70.43357022195683, 68.96286262350509, 126.63274822371433, 250.71274593435515, 117.23443339892656, 104.71857992547949, 178.68935615373013, 109.18580499665214, 159.71877724823864, 119.26658630686023, 105.00384800839824, 71.79696087262354, 60.969947989628075, 56.03867139105796, 202.68346342365706, 193.7557245661346, 156.68842000553457, 114.14657132099182, 94.92424828812915, 69.28981715173367, 61.78735874936664, 76.8385923660924, 58.68214761232326, 52.437186520669016, 51.171432209274286, 49.024313049664336, 11.328584261315582, 9.589031259692208, 52.73468663691891, 176.69019626918174, 3.6479884642767146, 34.47646385355753, 34.475640061887034, 106.83410027021714, 161.3267805966268, 2.368977746620543, 5.023699084540817, 26.378405663749053, 1.0595306700610094, 31.870445023535733, 62.80595329497106, 137.13549887911216, 18.6797957852436, 3.20674425559844, 124.50593923665937, 279.6969540769215, 243.88760166181757, 209.21752450105234, 120.37100123413992, 133.68544664039902, 150.44461349448414, 51.46560217155115, 98.19916050929591, 89.93813310822357, 74.50562707788235, 63.31213033754639, 63.16842162999189, 60.17067236700093, 54.59046026283657, 2457.8179000296177, 265.92714343342107, 97.04622725824862, 80.22423403156301, 43.522782941063305, 11.147962257005277, 7.902478990170837, 122.48164889832775, 1.5639228931903457, 1.1308507537122017, 24.623710930378156, 2.2849623893887543, 1.3656245903360893, 3.364788375072787, 23.46419595657423, 24.232023297243764, 1.746984172041712, 0.7615278482553954, 1.0668199652131527, 1.3216153844171095, 16.656923688041037, 0.7701912955420928, 1.1543966584501042, 2.458400659104243, 1.013245288296981, 0.5492538694508762, 1.16232571632526, 0.3815811524514275, 27.483168957225658, 0.8454320608506642, 21.791393118857837, 6.226937799492447, 38.20322959240554, 15.69355418361176, 16.680254825228822, 34.17146053657758, 53.25641612962156, 22.039134024322397, 155.39787803324964, 33.22236404776888, 20.023478581795207, 17.59915615258957, 57.06832297553417, 105.58436768415174, 74.29792313540032, 64.66536924936972, 89.1755761964906, 26.4284512990903, 46.01171171801618, 42.974127708367554, 32.097662938304374, 34.243774682711475, 42.53518018680509, 40.4968950921565, 29.907242380576083, 32.015160895465925, 22.125977306385096, 314.7976266980268, 266.117324355226, 102.17099847323564, 56.48395166753598, 11.286684033769802, 0.7337191488252834, 16.65003000705859, 0.7177154804124282, 0.7986808283654887, 1.3029294504921864, 0.6883101666098314, 0.6777099372248166, 1.1643234132535478, 24.551353534671335, 1.2145139499419646, 22.954687205576793, 0.47197465257501875, 0.9354228181607399, 0.6018871004283799, 13.46347582405333, 0.9045141920506579, 0.6335903075701909, 3.9821290779735468, 0.6215908379367051, 25.692294354518854, 14.004203004457597, 0.624196516497815, 3.85618449506735, 0.6344959394152905, 3.89956443214713, 20.25433021036421, 18.237520714234805, 78.46780596612122, 50.54898146360317, 21.361985695147755, 10.92502238514659, 19.049567684961918, 59.20584133238074, 18.231379410401114, 18.000905937005015, 17.914672460316915, 18.115238758853085, 119.68815222349808, 18.220030848078718, 91.89013328210677, 43.91165878694347, 57.90549069778409, 40.5182419181382, 29.785699338384582, 121.75306239632084, 75.7727997923441, 18.547967610887376, 61.71118135728321, 27.39345678503521, 27.01476254482708, 27.58604559192479, 27.295344803789135, 32.05679780563398, 98.76041076704487, 52.36790273048722, 43.93719526764644, 31.53608613581956, 24.137544861114566, 22.873788598885636, 23.73029754593942, 234.50295989940102, 161.2368539447804, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.19409068996745657, 0.1940906669644018, 0.1940906669644018, 0.1940906669644018, 0.1940906669644018, 0.1940906669644018, 0.1940906669644018, 0.1940906669644018, 0.1940906669644018, 0.1940906669644018, 0.1940906669644018, 0.1940906669644018, 7.797908959214161, 7.704494473903431, 7.792965326706798, 14.546138106442877, 14.546102773750745, 7.000562720768237, 11.682686932201333, 0.19409071297051136, 0.19409068996745657, 0.19409068996745657, 12.925240549553692, 7.663961251149123, 10.949397392393843, 7.440337698259493, 10.447239602553871, 10.563245663993765, 10.61724726732985, 10.608076225428535, 10.13510397538672, 10.319043026230945, 10.40286615782355, 15.578524037825584, 7.623639288449851, 7.4710808209024355, 7.471058737969853, 23.536730060261654, 7.785082455872768, 7.785078039286251, 7.78507509489524, 7.695929976550369, 7.6959248238660996, 8.83147706358776, 8.831466758219221, 14.88264961069182, 9.456184823889433, 41.31260011051109, 26.255973795960553, 13.027898214345505, 32.85336473608999, 44.32653174834408, 13.329350854827226, 14.012143352699399, 13.89073580594994, 37.85022888853941, 9.19608100247205, 16.692653624649424, 9.107001396827172, 11.56921451922635, 9.223500643761398], \"Total\": [2458.0, 588.0, 235.0, 315.0, 266.0, 162.0, 326.0, 266.0, 347.0, 238.0, 203.0, 194.0, 157.0, 102.0, 146.0, 144.0, 235.0, 114.0, 153.0, 622.0, 97.0, 477.0, 95.0, 165.0, 81.0, 57.0, 687.0, 282.0, 411.0, 77.0, 588.2880760127996, 326.51458691776224, 238.87878220888865, 146.8323771443998, 144.14187268706263, 76.93007781752677, 74.32512101072948, 71.92595742460796, 64.83453291289115, 60.08429929285721, 57.94869886461503, 54.17480649393427, 347.687278186173, 8.796655511951268, 153.81209425464016, 52.04205835233496, 136.85271750699104, 2.269352677950293, 86.89013594970878, 8.324442036761734, 81.75271717728752, 70.08985623988694, 10.892568878485646, 78.39168083961539, 58.90834756610932, 144.61738907776058, 67.62847938058161, 22.311899638189345, 142.60918162708174, 162.4712779530433, 169.98272515581814, 360.7803115458646, 784.7770254373376, 336.746075049544, 320.6159123026054, 687.7646167845596, 351.1948262540244, 622.1265619176237, 477.21185632936255, 411.98082842660114, 291.9082701926079, 189.257539642775, 235.70862399402867, 203.4572074616345, 194.57695230612626, 157.46006261487156, 114.9275901260658, 95.69625136004292, 70.0601880988653, 62.56786856224174, 77.81337600346984, 59.454436362141216, 53.21272239912946, 51.94596788641234, 49.80680554619744, 12.084386401792996, 10.39211915786423, 59.29962244928429, 235.70862399402867, 5.209620156360504, 50.0146649875513, 51.04843418709227, 158.99188633178616, 282.8475963362122, 4.71431059441382, 10.598010547742016, 62.345800938226375, 2.5170391508234284, 80.36152993532777, 162.4712779530433, 360.7803115458646, 50.09249817646568, 8.643126479078067, 336.746075049544, 784.7770254373376, 687.7646167845596, 622.1265619176237, 351.1948262540244, 411.98082842660114, 477.21185632936255, 140.21367387857177, 320.6159123026054, 291.9082701926079, 226.42863958294498, 189.257539642775, 198.09792004949355, 215.74577852399156, 169.98272515581814, 2458.6201552015823, 266.74092461083933, 97.84738120143022, 81.03565964922765, 44.403822573835164, 11.923596421550142, 8.676758724864515, 165.12750962694778, 2.341095947732286, 1.903987392541776, 58.778040123989975, 6.099665373607402, 3.7575474503967983, 9.59504643165112, 67.62847938058161, 71.52109335933652, 5.207505640663065, 2.365185039896618, 3.3483045795817556, 4.150720368486398, 52.39623486660284, 2.5064076253075576, 3.7814829085337536, 8.073667092488973, 3.368495789284727, 1.829057668854357, 3.940829540376002, 1.3135597177333902, 94.74718788753556, 2.988511368531272, 78.39168083961539, 22.311899638189345, 142.60918162708174, 57.54768923689275, 63.30434629776447, 140.21367387857177, 226.42863958294498, 91.370188125474, 784.7770254373376, 144.61738907776058, 85.04221143267084, 78.51094042733274, 320.6159123026054, 687.7646167845596, 477.21185632936255, 411.98082842660114, 622.1265619176237, 135.87734106353426, 291.9082701926079, 282.8475963362122, 189.257539642775, 215.74577852399156, 360.7803115458646, 351.1948262540244, 198.09792004949355, 336.746075049544, 136.9737623103867, 315.57282780038366, 266.89149778181724, 102.95595890531716, 57.25628654310408, 12.63023029509592, 2.405527943493121, 54.993482309938415, 2.4597996811038887, 2.784959212490014, 4.632484679910386, 2.4537548197932724, 2.435187150923246, 4.231286560454515, 93.18383286459849, 4.646914399476076, 90.6754616023734, 1.8800308424888947, 3.7814829085337536, 2.443291493367603, 55.62654770114899, 3.8248651640123086, 2.6883562697074725, 16.920092093863104, 2.6605564069765624, 109.97531852949555, 60.03585337321253, 2.679877896829813, 16.563655253475332, 2.7299824226513003, 16.837276326724446, 88.8733493705404, 80.37906062596393, 351.1948262540244, 226.42863958294498, 95.87932495819045, 48.48015661077118, 86.37184147169447, 282.8475963362122, 82.94121052673624, 81.85785078963251, 81.84070726478816, 82.92288571250589, 622.1265619176237, 83.48558226977617, 477.21185632936255, 215.74577852399156, 291.9082701926079, 198.09792004949355, 142.59025661943383, 687.7646167845596, 411.98082842660114, 85.16618981698036, 336.746075049544, 134.40173050891642, 134.3309742348219, 138.30918045026664, 136.9737623103867, 169.98272515581814, 784.7770254373376, 360.7803115458646, 320.6159123026054, 189.257539642775, 125.19242779703968, 120.13752968165389, 135.87536426159852, 235.26830734856048, 162.0107667045599, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045357905764, 0.9464045601771476, 0.9464045740238825, 0.9464044621297516, 0.9464044621297516, 0.9464044621297516, 0.9464044621297516, 0.9464044621297516, 0.9464044621297516, 0.9464044621297516, 0.9464044621297516, 0.9464044621297516, 0.9464044621297516, 0.9464044621297516, 53.077354198860256, 52.71519908236949, 53.78698740951176, 109.49735117716415, 109.49734806332596, 49.72519041476652, 88.0361683435302, 0.9464047107669414, 0.9464045864483466, 0.9464045864483466, 100.60378924247242, 56.987317589080476, 85.16618981698036, 55.87517174432257, 81.84070726478816, 82.92288571250589, 83.48558226977617, 83.48619263529066, 80.20790713007884, 81.85785078963251, 82.94121052673624, 134.3309742348219, 59.67219137891182, 58.50307809412866, 58.50307559639087, 215.74577852399156, 61.6546460680492, 61.65464775900318, 61.654644814612176, 60.86312049459533, 60.86312032202817, 71.26256958107034, 71.2625646446344, 134.40173050891642, 78.12231420048688, 477.21185632936255, 291.9082701926079, 120.13752968165389, 411.98082842660114, 622.1265619176237, 125.19242779703968, 138.30918045026664, 136.9737623103867, 687.7646167845596, 76.62907454353684, 320.6159123026054, 77.11290875903737, 198.09792004949355, 86.37184147169447], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.5191, -3.1089, -3.4224, -3.9109, -3.9296, -4.5623, -4.597, -4.63, -4.7359, -4.8122, -4.8488, -4.9173, -3.0817, -6.8105, -3.9582, -5.0809, -4.1308, -8.5202, -4.9105, -7.2606, -5.005, -5.2146, -7.097, -5.1864, -5.475, -4.6452, -5.459, -6.5946, -4.7413, -4.6403, -4.6614, -4.0537, -3.3707, -4.1308, -4.2437, -3.7093, -4.2019, -3.8216, -4.1136, -4.241, -4.6212, -4.7846, -4.869, -3.5175, -3.5625, -3.7749, -4.0917, -4.2761, -4.5908, -4.7054, -4.4874, -4.757, -4.8695, -4.894, -4.9368, -6.4018, -6.5685, -4.8639, -3.6547, -7.535, -5.2889, -5.2889, -4.1579, -3.7457, -7.9667, -7.215, -5.5566, -8.7713, -5.3675, -4.6891, -3.9082, -5.9017, -7.6639, -4.0048, -3.1954, -3.3324, -3.4858, -4.0386, -3.9337, -3.8155, -4.8882, -4.2421, -4.33, -4.5183, -4.6811, -4.6833, -4.732, -4.8293, -0.9109, -3.1347, -4.1427, -4.3331, -4.9446, -6.3066, -6.6507, -3.9099, -8.2707, -8.5949, -5.5142, -7.8915, -8.4063, -7.5045, -5.5624, -5.5302, -8.16, -8.9903, -8.6532, -8.439, -5.9051, -8.979, -8.5743, -7.8184, -8.7047, -9.3171, -8.5675, -9.6813, -5.4043, -8.8858, -5.6364, -6.889, -5.075, -5.9646, -5.9037, -5.1865, -4.7428, -5.6251, -3.6719, -5.2147, -5.721, -5.85, -4.6736, -4.0584, -4.4098, -4.5487, -4.2273, -5.4434, -4.889, -4.9573, -5.2491, -5.1844, -4.9676, -5.0167, -5.3198, -5.2517, -5.6211, -2.4924, -2.6604, -3.6177, -4.2104, -5.8208, -8.554, -5.432, -8.5761, -8.4692, -7.9798, -8.6179, -8.6334, -8.0922, -5.0436, -8.05, -5.1109, -8.9952, -8.3111, -8.7521, -5.6444, -8.3447, -8.7007, -6.8626, -8.7198, -4.9982, -5.605, -8.7157, -6.8947, -8.6993, -6.8835, -5.236, -5.3409, -3.8817, -4.3214, -5.1828, -5.8533, -5.2973, -4.1634, -5.3412, -5.354, -5.3588, -5.3476, -3.4595, -5.3419, -3.7238, -4.4622, -4.1856, -4.5426, -4.8503, -3.4424, -3.9166, -5.324, -4.1219, -4.9341, -4.948, -4.9271, -4.9377, -4.7769, -3.6517, -4.2861, -4.4616, -4.7932, -5.0606, -5.1144, -5.0776, -1.9082, -2.2828, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -9.0051, -5.3118, -5.3239, -5.3124, -4.6883, -4.6883, -5.4197, -4.9076, -9.0051, -9.0051, -9.0051, -4.8065, -5.3291, -4.9724, -5.3587, -5.0193, -5.0083, -5.0032, -5.004, -5.0497, -5.0317, -5.0236, -4.6198, -5.3344, -5.3546, -5.3546, -4.2071, -5.3135, -5.3135, -5.3135, -5.325, -5.325, -5.1873, -5.1873, -4.6655, -5.119, -3.6445, -4.0978, -4.7986, -3.8736, -3.5741, -4.7757, -4.7257, -4.7344, -3.732, -5.1469, -4.5507, -5.1566, -4.9173, -5.1439], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.255, 1.2539, 1.253, 1.2511, 1.2508, 1.2461, 1.2458, 1.2457, 1.2436, 1.2433, 1.2429, 1.2418, 1.2183, 1.1664, 1.1573, 1.1184, 1.1016, 0.8116, 0.7762, 0.7715, 0.7426, 0.6869, 0.6662, 0.6032, 0.6003, 0.532, 0.4782, 0.4515, 0.4499, 0.4205, 0.3542, 0.2093, 0.1152, 0.2011, 0.1373, -0.0915, 0.088, -0.1034, -0.1303, -0.1107, -0.1463, 0.1236, -0.1803, 1.3183, 1.3179, 1.3172, 1.3153, 1.314, 1.3111, 1.3096, 1.3095, 1.3091, 1.3075, 1.3071, 1.3063, 1.2576, 1.2417, 1.2048, 1.0339, 0.9658, 0.9501, 0.9296, 0.9246, 0.7607, 0.634, 0.5756, 0.462, 0.4569, 0.3973, 0.3717, 0.3548, 0.3357, 0.3306, 0.3272, 0.2905, 0.2854, 0.2324, 0.2514, 0.1967, 0.1678, 0.3199, 0.1389, 0.1448, 0.2106, 0.2271, 0.1792, 0.0452, 0.1863, 1.4331, 1.4303, 1.4252, 1.4233, 1.4134, 1.3661, 1.3399, 1.1346, 1.03, 0.9124, 0.5633, 0.4515, 0.4212, 0.3855, 0.3748, 0.3511, 0.3412, 0.3001, 0.2896, 0.289, 0.2874, 0.2534, 0.2469, 0.2443, 0.2321, 0.2304, 0.2124, 0.1972, 0.1958, 0.1707, 0.1532, 0.1572, 0.1162, 0.134, 0.0997, 0.0216, -0.0139, 0.0113, -0.186, -0.0375, -0.0128, -0.062, -0.2926, -0.4405, -0.4265, -0.4184, -0.5091, -0.2039, -0.4141, -0.4509, -0.3409, -0.4072, -0.7045, -0.7267, -0.4573, -0.9197, -0.3896, 1.9044, 1.904, 1.8993, 1.8933, 1.7944, 0.7195, 0.7121, 0.6751, 0.6579, 0.6384, 0.6358, 0.6278, 0.6165, 0.5731, 0.565, 0.5331, 0.5248, 0.51, 0.5059, 0.4882, 0.465, 0.4616, 0.4602, 0.4529, 0.4528, 0.4513, 0.4498, 0.4494, 0.4477, 0.4442, 0.4281, 0.4236, 0.4083, 0.4074, 0.4054, 0.4168, 0.3953, 0.343, 0.3919, 0.3923, 0.3878, 0.3858, 0.2587, 0.3848, 0.2595, 0.315, 0.2893, 0.3199, 0.341, 0.1755, 0.2137, 0.3827, 0.21, 0.3164, 0.303, 0.2947, 0.2938, 0.2387, -0.1658, -0.0231, -0.0806, 0.1149, 0.2608, 0.2483, 0.1619, 2.7824, 2.7808, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 1.2013, 0.8677, 0.8625, 0.8538, 0.7671, 0.767, 0.8251, 0.766, 1.2013, 1.2013, 1.2013, 0.7336, 0.7793, 0.7343, 0.7694, 0.7272, 0.7251, 0.7234, 0.7226, 0.717, 0.7146, 0.7096, 0.6312, 0.728, 0.7276, 0.7276, 0.5701, 0.7163, 0.7163, 0.7163, 0.7177, 0.7177, 0.6976, 0.6976, 0.585, 0.674, 0.3388, 0.3771, 0.5641, 0.2567, 0.1441, 0.5457, 0.4961, 0.4971, -0.1142, 0.6654, -0.1697, 0.6494, -0.0548, 0.5487]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 1, 4, 2, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 2, 4, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 2, 3, 4, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 3, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 4, 1, 2, 3, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 2, 3, 4, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 5, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 2, 2, 3, 4, 1, 2, 3, 4, 5, 2, 1, 1, 2, 3, 4, 1, 2, 3, 4, 5, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 3, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 1, 2, 3, 4, 5, 2, 3, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 2, 4, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 2, 1, 2, 3, 4, 2, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 1, 2, 1, 2, 3, 4, 5, 3, 1, 1], \"Freq\": [0.2158668768699187, 0.2158668768699187, 0.2158668768699187, 0.2158668768699187, 0.23633473784214284, 0.23633473784214284, 0.23633473784214284, 0.23633473784214284, 0.23215830648343774, 0.25659602295537853, 0.1588451570676153, 0.21993944824746733, 0.12218858235970408, 0.8646852454478483, 0.07686091070647541, 0.038430455353237705, 0.526241208263575, 0.2546328427081815, 0.22068179701375729, 0.9984240002181224, 0.9780585396127932, 0.9970348373777664, 0.32231212619131555, 0.3328797368861128, 0.1690817711167557, 0.1690817711167557, 0.005283805347398616, 0.4227999006977103, 0.4227999006977103, 0.44819133118024007, 0.26891479870814405, 0.26891479870814405, 0.3972921913720962, 0.3972921913720962, 0.22452179445757905, 0.28065224307197384, 0.1683913458431843, 0.21048918230398037, 0.12629350938238823, 0.26613103711954444, 0.26613103711954444, 0.26613103711954444, 0.26613103711954444, 0.570695991486949, 0.2282783965947796, 0.12840659808456353, 0.07133699893586863, 0.7678102970936066, 0.19195257427340165, 0.9909056799521329, 0.60064085711924, 0.24025634284769598, 0.12012817142384799, 0.060559261279914935, 0.1998455622237193, 0.7388229876149622, 0.24092203550793767, 0.24092203550793767, 0.24092203550793767, 0.24092203550793767, 0.25718239630666295, 0.3359445051755785, 0.14305770794558129, 0.19288679722999724, 0.07072515898433232, 0.31036903693210643, 0.3416906828610346, 0.11389689428701154, 0.2220989438596725, 0.008542267071525864, 0.3470966214901439, 0.3470966214901439, 0.23139774766009594, 0.21519656142423166, 0.21519656142423166, 0.21519656142423166, 0.21519656142423166, 0.48403584414292733, 0.26276231539187483, 0.2281883265245229, 0.02074439332041117, 0.3474428023631346, 0.3711995751742891, 0.095027091244618, 0.18411498928644737, 0.0029695966013943125, 0.24882102184632435, 0.27370312403095676, 0.16173366420011082, 0.2239389196616919, 0.08708735764621352, 0.4010673123629359, 0.29552328279374224, 0.28496887983682284, 0.010554402956919366, 0.21993549297466874, 0.6798006146489761, 0.07997654289987954, 0.9895470927333423, 0.9997477629066571, 0.25375368047626545, 0.25375368047626545, 0.25375368047626545, 0.25375368047626545, 0.06717398431562165, 0.5692111302534255, 0.15202533292482795, 0.2085928986642988, 0.3758612286429187, 0.3758612286429187, 0.3758612286429187, 0.3758612286429187, 0.16394341963854162, 0.32788683927708323, 0.32788683927708323, 0.9817893876098172, 0.23744867304880604, 0.2648465968621298, 0.17352018415105055, 0.1917854666932664, 0.13698961906661886, 0.24092205283243226, 0.2920267307059785, 0.16061470188828816, 0.19711804322653548, 0.10220935574709247, 0.2502945845310945, 0.27810509392343835, 0.15759288655661505, 0.20394373554385478, 0.11124203756937534, 0.43084538314662035, 0.38776084483195833, 0.12309868089903439, 0.05539440640456548, 0.2385383234542294, 0.2612562590212989, 0.15902554896948626, 0.21582038788715993, 0.13630761340241682, 0.22912854318496714, 0.2532473372044374, 0.15677216112655645, 0.21706914617523201, 0.13265336710708625, 0.2595198976542085, 0.3792983119561509, 0.1397414833522661, 0.17966762145291357, 0.059889207150971194, 0.2887779450136826, 0.2887779450136826, 0.1443889725068413, 0.22689695679646488, 0.06188098821721769, 0.9972223062062175, 0.23002435507620506, 0.26288497722994864, 0.1643031107687179, 0.19716373292246148, 0.13144248861497432, 0.2321096854696116, 0.25654228394009704, 0.15881189005815533, 0.2198933862343689, 0.12216299235242717, 0.3000673282082775, 0.318253226887567, 0.14548718943431635, 0.23641668283076406, 0.35907168604667156, 0.35907168604667156, 0.35907168604667156, 0.9919289169376274, 0.23483497433640524, 0.25831847177004574, 0.1526427333186634, 0.22309322561958497, 0.12915923588502287, 0.2466528267681238, 0.3083160334601548, 0.1575837504351902, 0.1986925548965442, 0.0890690763329336, 0.8709263206602885, 0.5252135617689279, 0.9927243611933828, 0.2095384976733626, 0.3087935755186396, 0.19851015569055405, 0.25365186560459685, 0.033085025948425675, 0.20852270802051637, 0.3127840620307746, 0.2780302773606885, 0.19114581568547334, 0.017376892335043033, 0.32749466252597703, 0.30566168502424523, 0.17778281679981608, 0.13723585858231419, 0.05302294536134866, 0.25879524247596325, 0.2700472095401356, 0.14627557183424011, 0.22503934128344633, 0.10126770357755084, 0.24936513714334135, 0.31432580312185887, 0.1550673962067837, 0.19278649258140676, 0.08591571951997475, 0.23956057125960908, 0.26351662838557, 0.1557143713187459, 0.21560451413364817, 0.131758314192785, 0.2466081932022491, 0.2466081932022491, 0.17072874914001862, 0.18969861015557624, 0.151758888124461, 0.9838012990925687, 0.4092839526984535, 0.4092839526984535, 0.4092839526984535, 0.3715783627976109, 0.24771890853174058, 0.24771890853174058, 0.9225404492992669, 0.24693619375862547, 0.32924825834483396, 0.2351773273891671, 0.17638299554187534, 0.011758866369458356, 0.9872197048347526, 0.22536098113193323, 0.3541386846358951, 0.1609721293799523, 0.26828688229992054, 0.9871282652099554, 0.23688437561631367, 0.26181957304960984, 0.16207878331642517, 0.2119491781830175, 0.12467598716648089, 0.23956232269389693, 0.26351855496328663, 0.15571550975103302, 0.21560609042450724, 0.13175927748164332, 0.22983426466367537, 0.3283346638052505, 0.24077875345718372, 0.1970007982831503, 0.4583867666984855, 0.19222670861549393, 0.3400934075504892, 0.39897740092347966, 0.39897740092347966, 0.39897740092347966, 0.2644465211632396, 0.2644465211632396, 0.2644465211632396, 0.2644465211632396, 0.3719745077198465, 0.3719745077198465, 0.3719745077198465, 0.5467296176759462, 0.5467296176759462, 0.22907792012361958, 0.2531913853997901, 0.15673752429510812, 0.21702118748553434, 0.12056732638085241, 0.24329066755884485, 0.27572942323335753, 0.17841315620981957, 0.1946325340470759, 0.1297550226980506, 0.2432906608863013, 0.27572941567114145, 0.17841315131662094, 0.19463252870904105, 0.1297550191393607, 0.9635095127657232, 0.031637625792307326, 0.0028761477993006664, 0.3091068070230753, 0.3459052364305843, 0.15455340351153765, 0.17663246115604306, 0.007359685881501794, 0.3520147744643626, 0.37973247324108406, 0.1191861047399023, 0.1441320336389516, 0.005543539755344292, 0.3840610338245054, 0.1920305169122527, 0.3840610338245054, 0.5508342491963345, 0.18361141639877818, 0.09180570819938909, 0.09180570819938909, 0.9988595686704076, 0.2449255468027672, 0.2449255468027672, 0.16956384009422346, 0.20724469344849533, 0.1507234134170875, 0.23930364787771796, 0.2734898832888205, 0.17093117705551283, 0.1880242947610641, 0.11965182393885898, 0.9879100886946605, 0.23489778660674404, 0.27404741770786806, 0.15659852440449604, 0.20879803253932805, 0.11744889330337202, 0.26144712483171395, 0.26144712483171395, 0.26144712483171395, 0.26144712483171395, 0.25486622860827346, 0.32525785365246324, 0.15777433199559784, 0.18447460356408363, 0.08010081470545737, 0.9937611139980399, 0.994331106254711, 0.3731513294627936, 0.3731513294627936, 0.3731513294627936, 0.3731513294627936, 0.9920781333988793, 0.2121200926355894, 0.4242401852711788, 0.2121200926355894, 0.2121200926355894, 0.20844088814438796, 0.41688177628877593, 0.31266133221658193, 0.9970782266485133, 0.40753867987684667, 0.40753867987684667, 0.40753867987684667, 0.24313479534741725, 0.26629049014240935, 0.16208986356494484, 0.21997910055242514, 0.10420062657746454, 0.992356560923844, 0.9956256914713595, 0.5993684576102768, 0.22017616810173432, 0.17124813074579337, 0.01223200933898524, 0.23342395313145883, 0.2723279453200353, 0.16858396615049806, 0.20748795833907452, 0.11671197656572942, 0.9981847999893514, 0.9848674671359829, 0.3663027247731586, 0.3663027247731586, 0.3663027247731586, 0.3663027247731586, 0.24329067250493663, 0.27572942883892815, 0.17841315983695352, 0.1946325380039493, 0.1297550253359662, 0.18868887395546488, 0.6729903171078248, 0.13837184090067425, 0.287537178009483, 0.32961578942550485, 0.133248936150736, 0.21039305708010947, 0.04207861141602189, 0.17643529851992837, 0.41702888741073973, 0.20851444370536987, 0.1924748711126491, 0.4059236015703933, 0.32356229110683526, 0.08236131046355807, 0.18825442391670416, 0.2245218100104498, 0.28065226251306225, 0.16839135750783735, 0.21048919688479667, 0.126293518130878, 0.19988056012799668, 0.31647755353599477, 0.21653727347199642, 0.23319398681599612, 0.03331342668799945, 0.26026346170127457, 0.3547725399726871, 0.15412249687338045, 0.17738626998634355, 0.055251461143287335, 0.2304079210173707, 0.2688092411869325, 0.16640572073476773, 0.2176074809608501, 0.11520396050868535, 0.2375814641445562, 0.7509271277426152, 0.01272757843631551, 0.28620659684440575, 0.3982004825661298, 0.13688141588210712, 0.18665647620287332, 0.2656558940526837, 0.32158345069535393, 0.3355653398560215, 0.06990944580333781, 0.5230146816711777, 0.1913468347577479, 0.28064202431136365, 0.012756455650516528, 0.23002435442400962, 0.2628849764845824, 0.164303110302864, 0.19716373236343682, 0.1314424882422912, 0.23695055517111802, 0.2843406662053416, 0.2685439625272671, 0.1895604441368944, 0.031593407356149066, 0.05059054132369452, 0.8937662300519364, 0.05059054132369452, 0.9220032795281994, 0.9094365454155935, 0.2393036580945785, 0.27348989496523257, 0.17093118435327034, 0.18802430278859739, 0.11965182904728924, 0.3346147552021721, 0.3346147552021721, 0.3346147552021721, 0.3346147552021721, 0.24139001423489873, 0.29133277580073985, 0.16647587188613705, 0.1914472526690576, 0.10820931672598909, 0.26965541849885766, 0.2876324463987815, 0.14381622319939075, 0.23370136269900996, 0.05393108369977153, 0.23461514780145323, 0.26813159748737514, 0.16758224842960945, 0.20109869811553135, 0.13406579874368757, 0.28307199605865385, 0.47178666009775644, 0.1887146640391026, 0.9037000677585318, 0.09102015070949242, 0.2675444547159217, 0.3180245405113786, 0.15144025738637076, 0.20696835176137338, 0.0605761029545483, 0.41064605634966195, 0.41064605634966195, 0.41064605634966195, 0.181119442181734, 0.24149258957564534, 0.24149258957564534, 0.24149258957564534, 0.060373147393911336, 0.9966596995811907, 0.1854889031588896, 0.3312301842123029, 0.23406933017669404, 0.22523652526436597, 0.02649841473698423, 0.9622676422481652, 0.6214744563324738, 0.16112300719730802, 0.2071581521108246, 0.011508786228379144, 0.9819536999579171, 0.8549336990259118, 0.0584570050616008, 0.0292285025308004, 0.0365356281635005, 0.021921376898100302, 0.3435361347208773, 0.28628011226739775, 0.32445079390305076, 0.05725602245347954, 0.24566188243608814, 0.2754390803071291, 0.16377458829072541, 0.20099608562952664, 0.11910879148416394, 0.2211710355189967, 0.3402631315676872, 0.425328914459609, 0.017013156578384363, 0.24132637602603224, 0.2614369073615349, 0.16088425068402148, 0.20110531335502688, 0.14077371934851882, 0.38269809810073896, 0.345900204052591, 0.19134904905036948, 0.07359578809629595, 0.0073595788096295954, 0.24582605355127352, 0.29643729987065337, 0.1590639170037652, 0.20244498527751936, 0.10122249263875968, 0.3311640372473284, 0.24200448875766306, 0.22926741040199658, 0.1655820186236642, 0.025474156711332952, 0.23744866629634365, 0.2648465893305371, 0.17352017921655882, 0.19178546123935447, 0.13698961517096747, 0.24553255285511916, 0.27529407441331544, 0.16368836857007943, 0.20089027051782476, 0.11160570584323598, 0.2476188100629919, 0.29554503136550647, 0.15975407100838188, 0.19170488521005827, 0.10384014615544823, 0.9909239586501682, 0.8543007397613539, 0.415709159689859, 0.415709159689859, 0.415709159689859, 0.44877895847798827, 0.26646250659630555, 0.26646250659630555, 0.021036513678655697, 0.2986586125103685, 0.2986586125103685, 0.2986586125103685, 0.990714875413901, 0.23640533265482608, 0.3546079989822391, 0.17730399949111955, 0.23640533265482608, 0.2781469090794589, 0.3637305734116001, 0.2424870489410667, 0.1069795804151765, 0.9871262412380324, 0.14547178436369435, 0.34549548786377404, 0.2000237035000797, 0.30912754177285046, 0.4065371695435086, 0.4065371695435086, 0.9783145234849006, 0.24169414622580374, 0.26028600362778864, 0.16732671661786414, 0.20451043142183392, 0.14873485921587923, 0.31983607045596635, 0.3567892419429107, 0.19750833036125412, 0.12615048197267198, 0.2326614772565942, 0.2505585139686399, 0.17897036712045705, 0.19686740383250279, 0.12527925698431994, 0.2154816337693151, 0.6660341407415193, 0.0979461971678705, 0.9977528077410542, 0.34418264849476277, 0.3128933168134207, 0.11472754949825426, 0.2190253217693945, 0.9102654974991446, 0.24849958623075027, 0.2683795531292103, 0.1689797186369102, 0.1888596855353702, 0.12921978483999014, 0.29686841324873436, 0.29686841324873436, 0.29686841324873436, 0.22812093198945324, 0.26321645998783066, 0.17547763999188712, 0.19302540399107582, 0.1403821119935097, 0.9836286425199733, 0.99951031471732, 0.9772099162671425, 0.17817608630906312, 0.2969601438484385, 0.2375681150787508, 0.2375681150787508, 0.0593920287696877, 0.9913397661641471, 0.9963212211617849, 0.44065429305735426], \"Term\": [\"able\", \"able\", \"able\", \"able\", \"academy\", \"academy\", \"academy\", \"academy\", \"access\", \"access\", \"access\", \"access\", \"access\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"algorithms\", \"algorithms\", \"also\", \"amazon\", \"analysis\", \"analyze\", \"analyze\", \"analyze\", \"analyze\", \"analyze\", \"application\", \"application\", \"applications\", \"applications\", \"applications\", \"apply\", \"apply\", \"array\", \"array\", \"array\", \"array\", \"array\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"available\", \"available\", \"back\", \"base\", \"base\", \"base\", \"case\", \"case\", \"case\", \"challenge\", \"challenge\", \"challenge\", \"challenge\", \"column\", \"column\", \"column\", \"column\", \"column\", \"columns\", \"columns\", \"columns\", \"columns\", \"columns\", \"common\", \"common\", \"common\", \"complete\", \"complete\", \"complete\", \"complete\", \"convert\", \"convert\", \"convert\", \"convert\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correctly\", \"correctly\", \"correctly\", \"correctly\", \"correctly\", \"could\", \"could\", \"could\", \"could\", \"cover\", \"cover\", \"cover\", \"create\", \"data\", \"deep\", \"deep\", \"deep\", \"deep\", \"describe\", \"describe\", \"describe\", \"describe\", \"description\", \"description\", \"description\", \"description\", \"development\", \"development\", \"development\", \"different\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"documentation\", \"documentation\", \"documentation\", \"documentation\", \"documentation\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"dont\", \"dont\", \"dont\", \"dont\", \"doors\", \"doors\", \"doors\", \"doors\", \"doors\", \"either\", \"either\", \"either\", \"either\", \"either\", \"engineer\", \"engineer\", \"engineer\", \"engineer\", \"engineer\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluate\", \"example\", \"excel\", \"excel\", \"excel\", \"excel\", \"excel\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"explore\", \"explore\", \"explore\", \"explore\", \"face\", \"face\", \"face\", \"feature\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"file\", \"file\", \"file\", \"file\", \"file\", \"finally\", \"find\", \"first\", \"five\", \"five\", \"five\", \"five\", \"five\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"format\", \"format\", \"format\", \"format\", \"format\", \"four\", \"four\", \"four\", \"four\", \"four\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"function\", \"function\", \"function\", \"function\", \"function\", \"generally\", \"generally\", \"generally\", \"generally\", \"generally\", \"go\", \"guide\", \"guide\", \"guide\", \"handle\", \"handle\", \"handle\", \"help\", \"however\", \"however\", \"however\", \"however\", \"however\", \"identify\", \"important\", \"important\", \"important\", \"important\", \"include\", \"infer\", \"infer\", \"infer\", \"infer\", \"infer\", \"info\", \"info\", \"info\", \"info\", \"info\", \"information\", \"information\", \"information\", \"information\", \"instance\", \"instance\", \"instance\", \"instead\", \"instead\", \"instead\", \"intelligence\", \"intelligence\", \"intelligence\", \"intelligence\", \"introduction\", \"introduction\", \"introduction\", \"isnt\", \"isnt\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"javascript\", \"javascript\", \"javascript\", \"javascript\", \"javascript\", \"json\", \"json\", \"json\", \"json\", \"json\", \"know\", \"know\", \"know\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"label\", \"label\", \"label\", \"label\", \"label\", \"language\", \"language\", \"language\", \"large\", \"large\", \"large\", \"large\", \"learn\", \"least\", \"least\", \"least\", \"least\", \"least\", \"library\", \"library\", \"library\", \"library\", \"library\", \"like\", \"line\", \"line\", \"line\", \"line\", \"line\", \"list\", \"list\", \"list\", \"list\", \"load\", \"load\", \"load\", \"load\", \"load\", \"look\", \"machine\", \"major\", \"major\", \"major\", \"major\", \"make\", \"manage\", \"manage\", \"manage\", \"manage\", \"management\", \"management\", \"management\", \"many\", \"methods\", \"methods\", \"methods\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"model\", \"module\", \"move\", \"move\", \"move\", \"move\", \"name\", \"name\", \"name\", \"name\", \"name\", \"need\", \"next\", \"nlpbased\", \"nlpbased\", \"nlpbased\", \"nlpbased\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"number\", \"number\", \"number\", \"numeric\", \"numeric\", \"numeric\", \"numeric\", \"numeric\", \"object\", \"object\", \"object\", \"object\", \"often\", \"often\", \"often\", \"often\", \"onedimensional\", \"onedimensional\", \"onedimensional\", \"onedimensional\", \"onedimensional\", \"overall\", \"overall\", \"overall\", \"overall\", \"overall\", \"pandas\", \"pandas\", \"pandas\", \"pandas\", \"pandas\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"part\", \"part\", \"part\", \"pass\", \"pass\", \"pass\", \"pass\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"perform\", \"perform\", \"perform\", \"perform\", \"pickle\", \"pickle\", \"pickle\", \"pickle\", \"pickle\", \"predictions\", \"predictions\", \"predictions\", \"predictions\", \"predictions\", \"problem\", \"problem\", \"problem\", \"problems\", \"process\", \"python\", \"python\", \"python\", \"python\", \"python\", \"recognize\", \"recognize\", \"recognize\", \"recognize\", \"reformat\", \"reformat\", \"reformat\", \"reformat\", \"reformat\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"require\", \"require\", \"require\", \"review\", \"review\", \"row\", \"row\", \"row\", \"row\", \"row\", \"scientists\", \"scientists\", \"scientists\", \"seasonal\", \"seasonal\", \"seasonal\", \"seasonal\", \"seasonal\", \"section\", \"series\", \"series\", \"series\", \"series\", \"series\", \"service\", \"single\", \"single\", \"single\", \"single\", \"solve\", \"source\", \"source\", \"source\", \"source\", \"source\", \"speak\", \"speak\", \"speak\", \"speak\", \"spreadsheet\", \"spreadsheet\", \"spreadsheet\", \"spreadsheet\", \"spreadsheet\", \"start\", \"start\", \"start\", \"start\", \"statistics\", \"statistics\", \"statistics\", \"statistics\", \"statistics\", \"store\", \"store\", \"store\", \"store\", \"store\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"symptoms\", \"symptoms\", \"symptoms\", \"symptoms\", \"symptoms\", \"table\", \"table\", \"table\", \"table\", \"table\", \"tabular\", \"tabular\", \"tabular\", \"tabular\", \"tabular\", \"take\", \"takeaways\", \"terminology\", \"terminology\", \"terminology\", \"text\", \"text\", \"text\", \"text\", \"thank\", \"thank\", \"thank\", \"thats\", \"theres\", \"theres\", \"theres\", \"theres\", \"think\", \"think\", \"think\", \"think\", \"time\", \"tool\", \"tool\", \"tool\", \"tool\", \"traditional\", \"traditional\", \"train\", \"try\", \"try\", \"try\", \"try\", \"try\", \"type\", \"type\", \"type\", \"type\", \"typically\", \"typically\", \"typically\", \"typically\", \"typically\", \"understand\", \"understand\", \"understand\", \"use\", \"value\", \"value\", \"value\", \"value\", \"video\", \"visualize\", \"visualize\", \"visualize\", \"visualize\", \"visualize\", \"watch\", \"watch\", \"watch\", \"ways\", \"ways\", \"ways\", \"ways\", \"ways\", \"welcome\", \"well\", \"work\", \"year\", \"year\", \"year\", \"year\", \"year\", \"youll\", \"youre\", \"youve\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 5, 4, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el524001396966012831842814629242\", ldavis_el524001396966012831842814629242_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el524001396966012831842814629242\", ldavis_el524001396966012831842814629242_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el524001396966012831842814629242\", ldavis_el524001396966012831842814629242_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.040075 -0.139192       1        1  28.470928\n",
       "1     -0.022042 -0.062236       2        1  26.656343\n",
       "4     -0.168090  0.131101       3        1  23.849784\n",
       "3      0.045888 -0.015345       4        1  14.853893\n",
       "0      0.184319  0.085672       5        1   6.169052, topic_info=         Term         Freq        Total Category  logprob  loglift\n",
       "37       data  2458.000000  2458.000000  Default  30.0000  30.0000\n",
       "26       well   588.000000   588.000000  Default  29.0000  29.0000\n",
       "4       learn   235.000000   235.000000  Default  28.0000  28.0000\n",
       "44       need   315.000000   315.000000  Default  27.0000  27.0000\n",
       "70    section   266.000000   266.000000  Default  26.0000  26.0000\n",
       "...       ...          ...          ...      ...      ...      ...\n",
       "484      line     9.196081    76.629075   Topic5  -5.1469   0.6654\n",
       "492    format    16.692654   320.615912   Topic5  -4.5507  -0.1697\n",
       "1174     name     9.107001    77.112909   Topic5  -5.1566   0.6494\n",
       "279       row    11.569215   198.097920   Topic5  -4.9173  -0.0548\n",
       "224      miss     9.223501    86.371841   Topic5  -5.1439   0.5487\n",
       "\n",
       "[319 rows x 6 columns], token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "31        1  0.215867     able\n",
       "31        2  0.215867     able\n",
       "31        3  0.215867     able\n",
       "31        4  0.215867     able\n",
       "2         1  0.236335  academy\n",
       "...     ...       ...      ...\n",
       "1219      4  0.237568     year\n",
       "1219      5  0.059392     year\n",
       "30        3  0.991340    youll\n",
       "348       1  0.996321    youre\n",
       "318       1  0.440654    youve\n",
       "\n",
       "[625 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 5, 4, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(num_topics))\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b71a13339a0be9489ff337af97259fe0ed71e682663adc836bae31ac651d564e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
